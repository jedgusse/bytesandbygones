{"cells":[{"cell_type":"markdown","id":"1d7171b6-43e0-4433-8a59-214981385a6f","metadata":{"id":"1d7171b6-43e0-4433-8a59-214981385a6f"},"source":["# 3. Unsupervised Analysis\n","\n","Once medieval texts are converted to a numerical format, the ensuing ways to manipulate and analyse them are numerous. In general, two general approaches can be discerned, parallel to a classic data-scientific distinction, (a) non-supervised and (b) supervised. The latter will be subject of our next notebook.\n","\n","Unsupervised methods allow to inspect data and enable a detection of trends on an **exploratory** basis. As opposed to supervised learning, it needs no labelled training input in order to be operational. Its aim is to describe without prescience, not to classify. By implication, the general disadvantage of unsupervised methods is that there is limited possibility of evaluation (a degree-of-error) for each observation, nor can it give estimations of which features are redundant for certain questions. Therefore, practitioners strongly advise to back up unsupervised analysis with supervised evidence when that proves feasible.\n","\n","Examples of unsupervised techniques applied to medieval texts are numerous. Popularly used are **principal components analysis** or **PCA** (Kestemont et al. 2015), ***k* nearest neighbours** (sometimes visualized with network analysis; e.g. Eder 2016:72), **t-distributed stochastic neighbor embedding** or t-SNE (Leclercq and Kestemont 2021:229) and **dendrogram analysis** (e.g. Dockray-Miller et al. 2021).\n","\n","Before we proceed, we **repeat**, with the block of code below, **the steps from the previous notebook.**"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"9MbsXZVxiwG9"},"id":"9MbsXZVxiwG9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9f64caf9-5bf1-4de2-ad37-15ab11556ee0","metadata":{"id":"9f64caf9-5bf1-4de2-ad37-15ab11556ee0"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from string import punctuation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# Lists to store metadata and text segments\n","authors = []\n","titles = []\n","texts = []\n","\n","sample_len = 1500  # Number of words per segment\n","\n","# Process all uploaded files\n","for filename in uploaded.keys():\n","    author = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n","    title = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n","\n","    bulk = []\n","    text = open(filename, encoding='utf-8-sig').read()  # Read file and handle BOM\n","\n","    # Clean text: remove digits, punctuation, lowercase\n","    for word in text.strip().split():\n","        word = re.sub(r'[\\d%s]', '', word)\n","        word = re.sub('[%s]' % re.escape(punctuation), '', word)\n","        word = word.lower()\n","        bulk.append(word)\n","\n","    bulk = [word for word in bulk if word != \"\"]  # Remove empty strings\n","    bulk = [bulk[i:i+sample_len] for i in range(0, len(bulk), sample_len)]  # Split into segments\n","\n","    # Keep only full-length segments\n","    for index, sample in enumerate(bulk):\n","        if len(sample) == sample_len:\n","            authors.append(author)\n","            titles.append(f\"{title}_{index+1}\")\n","            texts.append(\" \".join(sample))\n","\n","# Vectorize text by most frequent words\n","model = CountVectorizer(max_features=250, analyzer='word', ngram_range=(1,1))\n","X = model.fit_transform(texts).toarray()\n","\n","# Sort features by frequency\n","feat_frequencies = np.asarray(X.sum(axis=0)).flatten()\n","features = model.get_feature_names_out()\n","feat_freq_df = pd.DataFrame({'feature': features, 'frequency': feat_frequencies})\n","feat_freq_df = feat_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n","sorted_features = feat_freq_df['feature'].tolist()\n","\n","# Refit vectorizer with sorted vocabulary\n","model = CountVectorizer(analyzer='word', vocabulary=sorted_features, ngram_range=(1,1))\n","X = model.fit_transform(texts).toarray()\n","\n","# Scale features for uniform variance\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)"]},{"cell_type":"markdown","id":"ba2782f4-c9da-4c0d-8dac-916eb2f5ef8d","metadata":{"id":"ba2782f4-c9da-4c0d-8dac-916eb2f5ef8d"},"source":["## 3.1 Principal Components Analysis (PCA)"]},{"cell_type":"markdown","id":"3152bf40-4b70-4878-9ac2-7ea5e6790421","metadata":{"id":"3152bf40-4b70-4878-9ac2-7ea5e6790421"},"source":["Principal Components Analysis (**PCA**) is a commonly and popularly applied technique to reproject high-dimensional data in an informative and visualizable lower dimensional space. It maximally captures the features’ variance **in 2 or 3 \"principal components\" (PC’s)** or **\"composite features\"**. Although PC’s do not capture the full lexical information available in the data, they summarize it by its most informative trends and weed out redundant features (i.e. features with a low variance). The first component explains the largest proportion of the variance in the data set. The second component is orthogonal to the first and explains the largest proportion of the remaining variance. The third component is orthogonal to the second, and this pattern continues with each subsequent component (also see Craig, \"Principal Components Analysis\", 2024).\n","\n","PCA centers the features’ frequencies, where the mean shifts to zero, and where frequencies are recast to their number of standard deviations removed from the mean (this is how the x-, y- and z-axis, respectively PC 1, PC 2, and PC 3, should thus be read in the figure you will learn to plot below). The data point, i.e. a text segment, is apportioned a position in space by coordinates that correspond to its distance (i.e. number of standard deviations) away from the means of PC’s 1 and 2. Put differently, the higher the absolute coordinate values of the data points, the further they are removed from the intersection point (0,0), and the \"more deviant\" their lexical composition is in terms of the predefined features — i.e. outliers.\n","\n","Also here, Scikit-Learn provides us with a helpful module: `sklearn.decomposition.PCA`.\n","\n","The original variables’ contribution to the principal components is indicated by `loadings` (variables `l1, l2, l3` below), i.e. weights for each PC, and normalized to `[-1, 1]`. They are plotted on top of scatterplot. Higher absolute loadings indicate higher importance in explaining the variability of the original data."]},{"cell_type":"code","execution_count":null,"id":"e8ad039a-528c-48ce-b242-54dc121cd41c","metadata":{"scrolled":true,"id":"e8ad039a-528c-48ce-b242-54dc121cd41c"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","# -------------------------------------------------------\n","# PCA: reduce features to 3 principal components\n","# -------------------------------------------------------\n","pca = PCA(n_components=3)\n","X_bar = pca.fit_transform(X)\n","\n","# Explained variance per PC\n","var_exp = pca.explained_variance_ratio_\n","var_pc1, var_pc2, var_pc3 = np.round(var_exp[:3]*100, 2)\n","\n","# PCA loadings (feature contributions)\n","loadings = pca.components_.T\n","\n","# Sort features by contribution per PC\n","vocab_weights_p1 = sorted(zip(features, loadings[:,0]), key=lambda t: t[1], reverse=True)\n","vocab_weights_p2 = sorted(zip(features, loadings[:,1]), key=lambda t: t[1], reverse=True)\n","vocab_weights_p3 = sorted(zip(features, loadings[:,2]), key=lambda t: t[1], reverse=True)\n","\n","# -------------------------------------------------------\n","# 3D scatterplot setup\n","# -------------------------------------------------------\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","x1, x2, x3 = X_bar[:,0], X_bar[:,1], X_bar[:,2]\n","\n","# Rescale loadings to plot range\n","scaler_one = MinMaxScaler(feature_range=(min(x1), max(x1)))\n","scaler_two = MinMaxScaler(feature_range=(min(x2), max(x2)))\n","scaler_three = MinMaxScaler(feature_range=(min(x3), max(x3)))\n","\n","realigned_l1 = scaler_one.fit_transform(loadings[:,0].reshape(-1,1)).flatten()\n","realigned_l2 = scaler_two.fit_transform(loadings[:,1].reshape(-1,1)).flatten()\n","realigned_l3 = scaler_three.fit_transform(loadings[:,2].reshape(-1,1)).flatten()\n","\n","# Normalize loadings for opacity\n","normalized_l1 = (np.abs(loadings[:,0]) - min(np.abs(loadings[:,0]))) / (max(np.abs(loadings[:,0])) - min(np.abs(loadings[:,0])))\n","normalized_l2 = (np.abs(loadings[:,1]) - min(np.abs(loadings[:,1]))) / (max(np.abs(loadings[:,1])) - min(np.abs(loadings[:,1])))\n","normalized_l3 = (np.abs(loadings[:,2]) - min(np.abs(loadings[:,2]))) / (max(np.abs(loadings[:,2])) - min(np.abs(loadings[:,2])))\n","\n","# Compute feature ranks per PC, weighted by explained variance\n","d = {feat: [] for feat, _ in zip(features, normalized_l1)}\n","for idx, (feat, _) in enumerate(zip(features, normalized_l1)):\n","    d[feat].append(idx * var_pc1)\n","for idx, (feat, _) in enumerate(zip(features, normalized_l2)):\n","    d[feat].append(idx * var_pc2)\n","for idx, (feat, _) in enumerate(zip(features, normalized_l3)):\n","    d[feat].append(idx * var_pc3)\n","\n","# Select top discriminating features\n","n_top_discriminants = 20\n","best_discriminants = sorted([[feat, np.average(ranks)] for feat, ranks in d.items()], key=lambda x: x[1])\n","top_discriminants = [i[0] for i in best_discriminants][:n_top_discriminants]\n","\n","# -------------------------------------------------------\n","# Assign random color per author\n","# -------------------------------------------------------\n","def generate_random_color():\n","    return [random.random() for _ in range(3)]\n","\n","color_dict = {author: generate_random_color() for author in set(authors)}\n","\n","# Plot data points\n","for p1, p2, p3, a, title in zip(x1, x2, x3, authors, titles):\n","    color = color_dict[a]\n","    ax.scatter(p1, p2, p3, marker='o', color=color, s=20, zorder=3, alpha=1, edgecolors='k', linewidth=0.3)\n","    ax.scatter(p1, p2, p3, marker='o', color=color, s=500, zorder=1, alpha=0.03)  # light \"cloud\" background\n","\n","# Plot top features with opacity scaled by importance\n","for x, y, z, opac_l1, opac_l2, opac_l3, feat in zip(realigned_l1, realigned_l2, realigned_l3, normalized_l1, normalized_l2, normalized_l3, features):\n","    total_opac = (opac_l1 + opac_l2 + opac_l3)/3\n","    if feat in top_discriminants:\n","        ax.text(x, y, z, feat, color='k', ha='center', va='center', fontdict={'size': 17*total_opac}, zorder=10000, alpha=total_opac)\n","\n","# Axes labels with explained variance\n","ax.set_xlabel(f'PC 1: {var_pc1}%')\n","ax.set_ylabel(f'PC 2: {var_pc2}%')\n","ax.set_zlabel(f'PC 3: {var_pc3}%')\n","\n","# Create legend\n","handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=author)\n","           for author, color in color_dict.items()]\n","ax.legend(handles=handles)\n","\n","plt.show()\n"]},{"cell_type":"markdown","id":"79471bbe-9240-4e9a-8f54-36521c602f2e","metadata":{"id":"79471bbe-9240-4e9a-8f54-36521c602f2e"},"source":["## 3.2 Dendrogram Analysis\n","\n","Hierarchical clustering, also known as “agglomerative clustering”.\n","\n","The code and explanation below is cited from *Humanities Data Analysis: Case Studies with Python*, which \"gives a practical guide to data-intensive humanities research using the Python programming language.\" The book is written by Folgert Karsdorp, Mike Kestemont and Allen Riddell, and is available as an Open Access interactive Jupyter Book here: https://www.humanitiesdataanalysis.org/stylometry/notebook.html .\n","\n","\"*This clustering method works bottom-up: it will first detect very low-level text clusters that are highly similar. These texts are then joined into clusters that eventually are merged with other clusters that have been detected. Such a model is often visualized using a tree-like graph or “dendrogram” (an example will be offered shortly), showing at which stages subsequent clusters have been merged in the procedure. As such, it offers an efficient visualization from which the main structure in a dataset becomes clear at a glance. Because of the tree-with-branches metaphor, this model is strongly hierarchical in nature.*\" (M. Kestemont, [link](https://www.humanitiesdataanalysis.org/stylometry/notebook.html))"]},{"cell_type":"code","execution_count":null,"id":"1e411e21-5d43-44fc-b10f-c58f24b109dd","metadata":{"scrolled":true,"id":"1e411e21-5d43-44fc-b10f-c58f24b109dd"},"outputs":[],"source":["from scipy.spatial.distance import pdist\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","import matplotlib.pyplot as plt\n","from scipy.spatial.distance import pdist\n","from matplotlib import rcParams\n","from matplotlib import font_manager\n","\n","author_labels = []\n","for author, title in zip(authors, titles):\n","    first_part = author[:2]\n","    second_part = title.split('_')[-1]\n","    label = first_part + '_' + second_part\n","    author_labels.append(label)\n","\n","expected_classes = 4  # adjust to get the dendrogram color threshold right\n","\n","# 1. Calculate pairwise distances\n","dm = pdist(X, 'cityblock')\n","\n","# 2. Establish branch structure\n","linkage_matrix = linkage(dm, method='complete', optimal_ordering=True)\n","\n","def plot_tree(linkage_matrix, labels, expected_classes=2, figsize=(18, 5), ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(figsize=figsize)\n","\n","    # Get merge distances (ascending)\n","    distances = np.sort(linkage_matrix[:, 2])\n","\n","    # Determine threshold to produce exactly `expected_classes` clusters:\n","    # cut just BELOW the (k-1)-th largest merge height.\n","    if expected_classes is None or expected_classes <= 1:\n","        # Monochrome: everything \"above\" threshold color (single color)\n","        color_threshold = 0.0\n","    else:\n","        # Number of merges equals n_samples - 1; guard against out-of-range k\n","        n_merges = len(distances)\n","        if expected_classes - 1 >= n_merges:\n","            # Too many clusters requested; fall back to monochrome\n","            color_threshold = 0.0\n","        else:\n","            # heights[-1] = largest, heights[-(k-1)] = (k-1)-th largest\n","            # subtract a tiny epsilon to be strictly below that height\n","            color_threshold = distances[-(expected_classes - 1)] - 1e-12\n","\n","    dendro = dendrogram(\n","        linkage_matrix,\n","        labels=labels,\n","        ax=ax,\n","        leaf_font_size=6,\n","        leaf_rotation=90,\n","        color_threshold=color_threshold,\n","        # Reuse an existing cluster color for above-threshold links so we don't add a 5th distinct color:\n","        above_threshold_color='C0',\n","    )\n","\n","    # Clean up look\n","    ax.xaxis.set_ticks_position('none')\n","    ax.yaxis.set_ticks_position('none')\n","    for s in ax.spines.values():\n","        s.set_visible(False)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_tree(linkage_matrix, labels=author_labels, expected_classes=expected_classes)\n"]},{"cell_type":"markdown","id":"70799c26-1f2a-4c5c-b85e-7ba6ba449660","metadata":{"id":"70799c26-1f2a-4c5c-b85e-7ba6ba449660"},"source":["\"*The tree should be read from bottom to top, where the original texts are displayed as the tree’s leaf nodes. When moving to the top, we see how these original nodes are progressively being merged in new nodes by vertical branches. Subsequently, these newly created, non-original nodes are eventually joined into higher-level nodes and this process continues until all nodes have been merged into a single root node at the top. On the vertical axis, we see numbers which we can read as the distance between the various nodes: the longer the branches that lead up to a node, the more distant the nodes that are being merged.*\" (M. Kestemont, [link](https://www.humanitiesdataanalysis.org/stylometry/notebook.html))"]},{"cell_type":"markdown","id":"5cde333c-fc52-4fa2-8f9a-9901e5854fc1","metadata":{"id":"5cde333c-fc52-4fa2-8f9a-9901e5854fc1"},"source":["## 3.3 T-SNE\n","\n","**T-SNE**, or t-distributed stochastic neighbor embedding, (Van der Maaten and Hinton, 2008), is a machine learning algorithm for non-linear dimensionality reduction, particularly well-suited for visualizing high-dimensional datasets.\n","\n","Say we have a pair of vectors, representing respectively text segment A and text segment B.\n","\n","t-SNE works by comparing how similar data points are in the original high-dimensional space with how similar they are in a lower-dimensional embedding (usually 2D or 3D).\n","\n","1. **Original space (high-dimensional)**: For each pair of segments, t-SNE calculates the probability that they are neighbors. These probabilities are called joint probabilities. If the data is very high-dimensional, it’s usually best to reduce it first with another method like PCA (for dense data) or TruncatedSVD (for sparse data) down to about 50 dimensions before running t-SNE.\n","2. **Embedding space (low-dimensional)**: t-SNE then defines a new set of probabilities describing how likely segments are to be neighbors in the 2D/3D map.\n","3. The algorithm adjusts the embedding so that the neighborhood probabilities in the low-dimensional space match those in the original space as closely as possible (it measures the difference using Kullback–Leibler (KL) divergence and tries to minimize it).\n","\n","Step 3 indicates that t-SNE has a (at times costly) learning process, with a number of **sensitive parameters** which heavily influence the resulting diagrammatic representation. In a way, the algorithm is 'smarter' than PCA, but also comes with configurable hyperparameters such as the **(1) perplexity**, **(2) learning rate**, and the **(3) number of iterations**, which need to be well understood.\n","\n","1. Perplexity (`perp`) controls the trade-off between capturing both the local and global aspects of the corpus. Smaller `perp`-values focus more on capturing the local structure (more clusters), while larger values will take a broader view (fewer clusters). Typically, the values range between 5 and 50.\n","2. The learning rate (`lr`) controls the step size at each iteration: i.e. how much the algorithm should change the positions of the points per iteration. If it is too high, the data may become too spread out, and if it is too low, the data may become too compressed. Both situations can lead to poor representations. A common heuristic is to start with a learning rate between 10 and 1000.\n","3. Number of Iterations (`n_iter`): number of times the algorithm updates the positions of the data points.\n","\n","In summary, t-SNE is a powerful tool for reducing high-dimensional data to lower dimensions for visualization, but its effectiveness can be influenced significantly by the choice of hyperparameters. Perplexity, learning rate, and the number of iterations are crucial parameters that control the algorithm's behavior and the quality of the final embedding. Adjusting these parameters through experimentation and cross-validation is often necessary to obtain the best results for a given dataset.\n","\n","Note that the actual point positioning (coordinates) of t-SNE's derived features in the vectors is insignificant and even arbitrary: it retains the (visualizable) relationships between the data points, but the values in the vectors themselves are quite meaningless. So even though t-SNE has tighter clusters and sometimes clearer visualizationst than PCA, these plots are more abstract and less interpretable."]},{"cell_type":"code","execution_count":null,"id":"a6f20f5b-4ef1-4018-94bb-1b97268c2279","metadata":{"id":"a6f20f5b-4ef1-4018-94bb-1b97268c2279"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import random\n","\n","\"\"\"\n","Dimensionality reduction workflow using PCA + t-SNE.\n","\n","1. PCA reduces very high-dimensional input to ~50 dims.\n","   - Speeds up t-SNE optimization\n","   - Denoises and removes collinearity\n","2. t-SNE maps reduced data into 2D while preserving local/global structure.\n","   - Key hyperparameters:\n","     - perplexity: trade-off local vs. global structure (5–50 common)\n","     - learning_rate: step size in optimization\n","     - max_iter: number of optimization iterations\n","3. Visualization: points are colored per author with legend.\n","\"\"\"\n","\n","# --- Parameters ---\n","lr = 10           # learning rate for t-SNE optimization\n","max_iter = 1000   # maximum number of iterations\n","perp = 10         # low perplexity = local | high perplexity = global structure\n","\n","# --- Step 1: Reduce dimensionality with PCA before t-SNE ---\n","# PCA condenses features into 50 principal components\n","pca = PCA(n_components=50)\n","pca_X = pca.fit_transform(X)\n","\n","# --- Step 2: Compute t-SNE embedding ---\n","# Maps PCA-reduced data into 2D\n","tsne = TSNE(\n","    n_components=2,       # project into 2D for visualization\n","    perplexity=perp,      # balance local vs. global structure\n","    learning_rate=lr,     # optimization step size\n","    max_iter=max_iter     # iterations for convergence\n",")\n","tsne_X = tsne.fit_transform(pca_X)\n","\n","# --- Step 3: Plotting ---\n","# Create 2D scatter of t-SNE projection\n","fig = plt.figure(figsize=(4.7, 3.2))\n","ax = fig.add_subplot(111)\n","x1, x2 = tsne_X[:, 0], tsne_X[:, 1]\n","\n","# -------------------------------------------------------\n","# Assign random color per author\n","# -------------------------------------------------------\n","def generate_random_color():\n","    \"\"\"Generate a random RGB color as a list of floats.\"\"\"\n","    return [random.random() for _ in range(3)]\n","\n","color_dict = {author: generate_random_color() for author in set(authors)}\n","\n","# Scatter each point, colored by author, with black edge\n","for p1, p2, a in zip(x1, x2, authors):\n","    ax.scatter(\n","        p1, p2,\n","        marker='o',\n","        color=color_dict[a],  # color based on author\n","        s=40,                 # marker size\n","        edgecolors='k',       # black border around marker\n","        linewidths=0.3\n","    )\n","\n","# Label axes\n","ax.set_xlabel('t-SNE-1')\n","ax.set_ylabel('t-SNE-2')\n","\n","# --- Step 4: Create legend ---\n","# Each author gets a marker in the legend\n","handles = [\n","    plt.Line2D([0], [0], marker='o', color='w',\n","               markerfacecolor=color, markersize=10, label=author)\n","    for author, color in color_dict.items()\n","]\n","ax.legend(handles=handles)\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}