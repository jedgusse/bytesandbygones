{"cells":[{"cell_type":"markdown","id":"cd8258c1-4e65-48f2-9260-e89530f3f193","metadata":{"id":"cd8258c1-4e65-48f2-9260-e89530f3f193"},"source":["# 1. Preprocessing\n","\n","## 1.1 What is Preprocessing?\n","In this notebook, we survey some snippets of code that help 'preprocess' your corpus of texts.\n","\n","The preprocessing of text files is a significant first step in ensuring reliability of all later stylometric analyses. For all texts predating the invention of the printing press, one could argue that this is perhaps even slightly more important than for early modern or modern texts. Stylometry’s application to premodern texts comes with specific desiderata when compared to, for instance, authorship detection of current-day online, electronically available blog posts. As a result of scribal culture, premodern texts can be tremendously varied. These variations can be captured and in some cases they might even be desirable (e.g. when analyzing various recension of a text or comparing writing conventions across scriptoria), but more often than not they are redundant if what you want to analyze is individual writing style.\n","\n","Preprocessing (potentially) entails steps such as these:\n","\n","* the removal and editing of all irrelevant characters in the text: punctuation, numerals, optical character recognition errors, case-folding, titles or annotations, etc …\n","* ‘tokenising’ the text to meaningful units, often word tokens, but others are:\n","  * subwords; not necessarily linguistic yet meaningful word fragments that are picked up as relevant by a language model, e.g. un | believ | able\n","  * morphemes = linguistic meaningful units, e.g. un | believe | able\n","  * syllables (sounds, phonological units), e.g. un | be | liev | a | ble\n","\n","If so desired, preprocessing also takes care of this:\n","* normalization / standardization: align variant orthographical and editorial conventions between text versions\n","* disambiguation, for instance to semantically distinguish homographs (words that are spelled the same) and homonyms (words that sound and/or are spelled the same).\n","* stemming (recover basis stem or morphological root of word tokens\n","* lemmatisation transforms word tokens to a standard dictionary form\n","* PoS(part-of-speech)-tagging and parsing: identify a token’s part of speech and syntactic function\n","* automated scansion for prosodic units of analysis\n"]},{"cell_type":"markdown","id":"b9134155-9064-4973-8a0b-f02857ea2495","metadata":{"id":"b9134155-9064-4973-8a0b-f02857ea2495"},"source":["## 1.2 Reading and Handling File Objects\n","\n","The first step is reading in your corpus of texts, so that we can start manipulating them. The steps below will be easier to follow and execute correctly if you have ensured that your file names are formatted as such: ```author-name_text-title.txt```."]},{"cell_type":"code","source":["# Colab uploads the files into the temporary runtime (not your Drive).\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"zPhq_DsH3Wr9"},"id":"zPhq_DsH3Wr9","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1c6f147a-692b-4ddf-b0bd-41f9ae119c94","metadata":{"id":"1c6f147a-692b-4ddf-b0bd-41f9ae119c94"},"source":["Below is a rather large chunk of code in which a number of consecutive steps are introduced and combined.\n","First, we declare empty list containers (```authors```, ```titles```, ```texts```), where we will store our metadata and data.\n","We introduce our first stylometric parameter, the sample length (variable ```sample_len```)\n","We then go over all files you have uploaded, and extract the data (texts) and metadata (authors and titles) from the files.\n","\n","In the text itself, we use the ```re``` (RegEx Module in Python, which stands for regular expressions) which removes digits and punctuation from the text (if so desired). We also apply case folding (convert upper to lowercase).\n","\n","Once the data has been 'cleared' of some of the text items we could say are insignificant for stylistic analysis, we proceed by slicing up the data into discrete segments, or chunks of text."]},{"cell_type":"code","source":["import re\n","import glob\n","from string import punctuation\n","\n","# Declare empty lists to fill up with our metadata and data\n","authors, titles, texts = [], [], []\n","\n","# We declare some parameters — the 'settings' of our stylometric experiments\n","sample_len = 1400 # word length of text segment\n","\n","# Function to clean and split text\n","def clean_and_split_text(text, sample_len):\n","    words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()\n","    return [words[i:i + sample_len] for i in range(0, len(words), sample_len)]\n","\n","for filename in uploaded.keys():\n","    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n","    with open(filename, encoding='utf-8-sig') as file:\n","        text = file.read().strip()\n","        bulk = clean_and_split_text(text, sample_len)\n","\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:\n","                authors.append(author)\n","                titles.append(f\"{title}_{index + 1}\")\n","                texts.append(\" \".join(sample))\n","\n","# Print summary to confirm things worked\n","print(\"Text processing complete!\")\n","print(f\"Number of text segments: {len(texts)}\")\n","print(f\"Number of authors: {len(set(authors))}\")\n","print(f\"Number of titles/segments: {len(titles)}\")\n","\n","# Optional: print a sample segment\n","print(\"\\nSample processed text segment:\\n\", texts[0][:200], \"...\")"],"metadata":{"id":"alBqQ4Iv3uSr"},"id":"alBqQ4Iv3uSr","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f3aa3261-e91a-453a-a06e-be82a3d08d9a","metadata":{"id":"f3aa3261-e91a-453a-a06e-be82a3d08d9a"},"source":["## 1.3 Sampling (Text Segmentation)\n","\n","Despite many permutations, sampling methods generally fall within one of these four categories: (a) discrete, (b) rolling, (c) random and (d) generative.\n","\n","* **Discrete** is as above. A longer text is sliced in discrete pieces according to a predefined fixed sample size, where the next sample picks up the trail where the previous one ended.\n","* **Rolling**: makes use of a sliding window, it 'shingles' your text segments. Rolling segmentation samples the text in non-identical, partially overlapping windows instead of discrete chunks of text. It is generally considered to be a more sensitive way of linearly scanning the stylistic profile of a text, and registers how it changes from first to last word.\n","* **Random**: sentences from a certain author’s entire oeuvre are randomly selected until a predefined sample length limit is reached (e.g. keep on randomly selecting until 1,000 words have been found) in order to come to an almost inexhaustible number of new, real-world representations of the author’s lexical distribution through new combinations.\n","* **Generative**: closely related to random sampling, but takes the idea of inexhaustible representability of a stylistic profile one more step further. Text generation attempts to not only imitate the distribution by making use of extant text samples, but even expands a corpus by generating new text. Needless to say this is an interesting yet underexplored area of research for medieval texts. Some work has been done in this regard for Latin-writing late antique and medieval authors (Manjavacas et al. 2017)."]},{"cell_type":"markdown","source":["\n","### 1.3.1 Rolling Sampling\n","\n","The block of code below allows you to apply a relatively easy form of sampling, that of **rolling sampling**. The `step_size`-variable specifies the number of words between the starting indices of consecutive samples. For example, if `step_size=100`, each sample starts 100 words after the previous sample. It determines the amount of overlap between consecutive samples."],"metadata":{"id":"CLmssxjqKCA-"},"id":"CLmssxjqKCA-"},{"cell_type":"code","execution_count":null,"id":"faf30c8a-7bf3-45cb-87fc-a920064a1c21","metadata":{"id":"faf30c8a-7bf3-45cb-87fc-a920064a1c21"},"outputs":[],"source":["\"\"\"\n","Process uploaded text files into overlapping word-based samples.\n","Each sample is of fixed length (sample_len), taken in steps of step_size,\n","and stored along with its author and title metadata.\n","\"\"\"\n","\n","import re\n","import numpy as np\n","from string import punctuation\n","import pandas as pd\n","\n","# Declare empty lists to fill up with our metadata and data\n","authors = []\n","titles = []\n","texts = []\n","\n","sample_len = 1400 # word length of text segment\n","step_size = 200 # step size\n","\n","for filename in uploaded.keys():\n","    author, title = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:2]\n","    with open(filename, 'r') as file:\n","        text = file.read().lower()\n","        text = re.sub('[%s]' % re.escape(punctuation), '', text)\n","        text = re.sub(r'\\d+', '', text)\n","        words = text.split()\n","        steps = np.arange(0, len(words), step_size)\n","        for each_begin in steps:\n","            sample_range = range(each_begin, each_begin + sample_len)\n","            sample = [words[index] for index in sample_range if index < len(words)]\n","            if len(sample) == sample_len:\n","                key = '{}-{}-{}'.format(title, str(each_begin), str(each_begin + sample_len))\n","                authors.append(author)\n","                titles.append(key)\n","                texts.append(\" \".join(sample))\n","\n","# Turn results into a dataframe\n","df = pd.DataFrame({\n","    \"author\": authors,\n","    \"title\": titles,\n","    \"text\": texts\n","})\n","\n","# Peek at the data\n","print(df.head())\n","\n","# Count samples per author\n","author_counts = df[\"author\"].value_counts()"]},{"cell_type":"markdown","id":"cbb10930-fc5e-4f9d-9a74-049b60a17abb","metadata":{"id":"cbb10930-fc5e-4f9d-9a74-049b60a17abb"},"source":["### 1.3.2 Random sampling\n","The code block below gives you a starting point to experiment with **random sampling**. The variable `word_limit` is virtually the same as the desired `sample_len` above: it indicates how many words you want to include in your sample. `n_samples` yields the desired number of random samples per author."]},{"cell_type":"code","execution_count":null,"id":"dbeb2b44-d5dc-4aef-aeda-58c0bf27c21d","metadata":{"id":"dbeb2b44-d5dc-4aef-aeda-58c0bf27c21d"},"outputs":[],"source":["import re\n","from string import punctuation\n","import random\n","\n","# Upper word limit for each sample (in words)\n","word_limit = 1400\n","n_samples = 10  # number of randomly generated segments per author\n","\n","# Count the total number of words across a list of sentences\n","def count_words(sentences):\n","    return sum(len(sentence.split()) for sentence in sentences)\n","\n","# Randomly select sentences until the word limit is reached\n","def sample_sentences(sentences, word_limit):\n","    sampled_sentences = []\n","    total_words = 0\n","    remaining_sentences = sentences.copy()\n","\n","    while total_words < word_limit and remaining_sentences:\n","        sentence = random.choice(remaining_sentences)\n","        sentence_word_count = len(sentence.split())\n","        if total_words + sentence_word_count <= word_limit:\n","            sampled_sentences.append(sentence)\n","            total_words += sentence_word_count\n","        remaining_sentences.remove(sentence)\n","\n","    return sampled_sentences\n","\n","data = {}\n","# Read all uploaded files and split them into sentences\n","for filename in uploaded.keys():\n","    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n","    data[author] = []\n","    with open(filename, encoding='utf-8-sig') as file:\n","        text = file.read().strip()\n","        # Split sentences on . or ? while avoiding common abbreviation issues\n","        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n","        for sentence in sentences:\n","            data[author].append(sentence)\n","\n","# Generate random samples per author and collect metadata\n","authors, titles, texts = [], [], []\n","sampled_data = {}\n","for author in data.keys():\n","    sampled_data[author] = []\n","for author, sentences in data.items():\n","    for i in range(0, n_samples):\n","        random_sample = sample_sentences(sentences, word_limit)\n","        random_sample = ' '.join(random_sample)\n","        title = 'sample_' + str(i+1)  # label each random sample\n","        authors.append(author)\n","        titles.append(title)\n","        texts.append(random_sample)\n","\n","# Store results in a dataframe for inspection and analysis\n","df = pd.DataFrame({\n","    \"author\": authors,\n","    \"title\": titles,\n","    \"text\": texts\n","})\n","\n","# Display a few samples (results will change each run due to random selection)\n","print(df.head())\n","\n","# Quick overview: number of samples generated per author\n","author_counts = df[\"author\"].value_counts()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}