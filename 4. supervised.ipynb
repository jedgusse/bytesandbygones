{"cells":[{"cell_type":"markdown","id":"268ecd03-c139-4090-a3c4-90526a2dde66","metadata":{"id":"268ecd03-c139-4090-a3c4-90526a2dde66"},"source":["# 4. Supervised Analysis and Classifiers\n","\n","Supervised algorithms (or **supervised** models) observe the way in which the feature inputs `X` (e.g. function word frequencies of known texts) correlate with class outputs `y` (e.g. authorship / label of the text). A supervised model ‘observes’ correctly labelled, preclassified X-y pairs, in order to register meaningful correlations (e.g. between certain function word patterns X with certain authors y). This process is called ‘training,’ and the X-y pairs which the model trains on correspond to what is commonly referred to as `training data`. Consequently, once this learning process has taken place, the model can be confronted with `test data,` comprising previously unobserved and unclassified texts. On the basis of what it has observed, the supervised model can make a prediction (classification), and assign the unseen test data to a class, either by a hard decision or by outputting a probability score.\n","\n","Classification is a considerable field of research on its own. There are **many types** of classifiers, and it is not always clear which one will perform best, and why. Varying types of classifiers tend to react differently to different problems, have a variety of parametrization options and require other methods by which to optimize their performance during training. A big advantage of supervised machine-learning methods, ‘text classification’ (Sebastiani 2002), is the **possibility of evaluation**. By making different combinations of parameters, such as the feature set, the vector length (number of features), sample length, vectorization method, scaling method, etc., and evaluating how well they can be fitted to a class (author), scholars can finetune and optimize these parameters.\n","Before we proceed, we **repeat**, with the block of code below, some of **the steps from the previous notebook**.\n","\n","These are:\n","\n","1. Loading and segmentation of documents, containers `authors`, `titles`, `texts`\n","2. Vectorization of `texts` to matrix `X` containing vectors for all text segments\n","3. Scale `X` by applying `StandardScaler()`\n","\n","Note that, as opposed to the three previous notebooks, we are now introducing a **test corpus** of allegedly **unknown authorship**. These files can be found in our `'corpus/test/'` folder. Below, we will apply a classifier to attribute the text to one of the **known classes**, i.e. our training set from the `'corpus/train/'` folder."]},{"cell_type":"code","execution_count":1,"id":"b7fa8ceb-1f6f-4bff-b015-2ed6eae05410","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"b7fa8ceb-1f6f-4bff-b015-2ed6eae05410","executionInfo":{"status":"ok","timestamp":1758197835480,"user_tz":-120,"elapsed":37058,"user":{"displayName":"Jeroen De Gussem UGent","userId":"05508689320763448270"}},"outputId":"6fbcf2a8-67dd-413c-b5fe-43f5ffbd2b1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Upload training files...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-30a21cde-df89-43cb-aafd-f5c83f02112b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-30a21cde-df89-43cb-aafd-f5c83f02112b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos_cleaned.txt to Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos_cleaned (8).txt\n","Saving Bernardus-Claraevallensis_Sermones-super-cantica-canticorum_cleaned.txt to Bernardus-Claraevallensis_Sermones-super-cantica-canticorum_cleaned (8).txt\n","Saving Hildegardis-Bingensis_Liber-divinorum-operum_cleaned.txt to Hildegardis-Bingensis_Liber-divinorum-operum_cleaned (8).txt\n","Upload test files...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-29389439-861c-4cbb-9f47-d282be5a1fa8\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-29389439-861c-4cbb-9f47-d282be5a1fa8\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Hildegardis-Bingensis_Scivias.txt to Hildegardis-Bingensis_Scivias (2).txt\n"]}],"source":["from google.colab import files\n","from string import punctuation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# --- Parameters ---\n","sample_len = 5000  # length of each text segment in words\n","data_dict = {\"train\": [[], [], []], \"test\": [[], [], []]}\n","\n","def process_uploaded(uploaded, dict_key):\n","    \"\"\"Helper: clean, segment, and add uploaded texts to data_dict.\"\"\"\n","    authors, titles, texts = data_dict[dict_key]\n","\n","    for filepath, fileobj in uploaded.items():\n","        filename = filepath.split(\"/\")[-1]  # e.g. \"author_title.txt\"\n","\n","        # Extract author and title from filename\n","        author = filename.split(\".\")[0].split(\"_\")[0]\n","        title = filename.split(\".\")[0].split(\"_\")[1]\n","\n","        # Decode uploaded bytes\n","        text = fileobj.decode(\"utf-8-sig\")\n","\n","        # Tokenize and clean\n","        bulk = []\n","        for word in text.strip().split():\n","            word = re.sub(r'\\d+', '', word)  # remove digits\n","            word = re.sub('[%s]' % re.escape(punctuation), '', word)  # remove punctuation\n","            word = word.lower()\n","            if word != \"\":\n","                bulk.append(word)\n","\n","        # Split text into equal-length samples\n","        bulk = [bulk[i:i+sample_len] for i in range(0, len(bulk), sample_len)]\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:\n","                authors.append(author)\n","                titles.append(title + \"_{}\".format(index + 1))\n","                texts.append(\" \".join(sample))\n","\n","\n","# --- Phase 1: Upload training files ---\n","print(\"Upload training files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"train\")\n","\n","# --- Phase 2: Upload test files ---\n","print(\"Upload test files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"test\")\n","\n","# After both phases:\n","# data_dict[\"train\"] = [authors, titles, texts]\n","# data_dict[\"test\"]  = [authors, titles, texts]"]},{"cell_type":"code","execution_count":3,"id":"b7976688-fa6a-45da-8d77-e16fb7883b05","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7976688-fa6a-45da-8d77-e16fb7883b05","executionInfo":{"status":"ok","timestamp":1758197908978,"user_tz":-120,"elapsed":2490,"user":{"displayName":"Jeroen De Gussem UGent","userId":"05508689320763448270"}},"outputId":"5a36ea8a-cb23-4daa-c03d-fd1abfaf3152"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training vectors shape: (73, 250)\n","First 5 training vectors:\n"," [[-1.6679167  -1.2055242   0.02400279 ... -0.85138809 -0.1603732\n","  -0.15149293]\n"," [-1.46936238 -0.50842679  1.31510028 ... -0.48362874  0.3486374\n","  -1.11314372]\n"," [-1.02813057 -0.9097859   1.56870871 ... -0.11586939 -0.66938381\n","   0.32933246]\n"," [-1.29286966 -0.93091007  3.2286912  ... -0.85138809 -0.66938381\n","  -0.63231833]\n"," [-1.13843852 -0.17044017  1.49954278 ... -0.48362874 -0.1603732\n","  -1.11314372]]\n","\n","Test vectors shape: (29, 250)\n","First 5 test vectors:\n"," [[ 0.18525692  1.64623792 -1.10570751 ... -0.48362874 -1.17839441\n","   2.25263403]\n"," [ 0.11907215  2.17434201 -0.5293247  ... -0.48362874 -1.17839441\n","   1.77180864]\n"," [ 0.14113374  0.39991225 -1.4515372  ...  0.98740867 -1.17839441\n","   0.32933246]\n"," [ 0.89122782  1.85747955 -0.75987783 ... -0.11586939 -1.17839441\n","  -0.63231833]\n"," [ 0.2293801   2.21659034 -0.87515439 ... -0.48362874 -1.17839441\n","   2.25263403]]\n","\n","First 10 features in vectorizer: ['et' 'in' 'est' 'non' 'ad' 'quod' 'ut' 'qui' 'cum' 'sed']\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","\n","# ==========================================================\n","# Vectorize and scale the training and test sets\n","# ==========================================================\n","\n","for set, [authors, titles, texts] in data_dict.items():\n","    if set == 'train':  # key must be 'train'\n","        # Vectorize training texts (bag-of-words, top 250 unigrams)\n","        model = CountVectorizer(\n","            max_features=250,\n","            analyzer='word',\n","            ngram_range=(1, 1)\n","        )\n","        X_train = model.fit_transform(texts).toarray()\n","\n","        # Sort features by frequency for consistent ordering\n","        feat_frequencies = np.asarray(X_train.sum(axis=0)).flatten()\n","        features = model.get_feature_names_out()\n","        feat_freq_df = pd.DataFrame({'feature': features, 'frequency': feat_frequencies})\n","        feat_freq_df = feat_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n","        sorted_features = feat_freq_df['feature'].tolist()\n","        sorted_indices = [model.vocabulary_[feat] for feat in sorted_features]\n","        X_train_sorted = X_train[:, sorted_indices]\n","\n","        # Refit vectorizer with sorted vocabulary\n","        model = CountVectorizer(\n","            stop_words=[],\n","            analyzer='word',\n","            vocabulary=sorted_features,\n","            ngram_range=(1, 1)\n","        )\n","        X_train = model.fit_transform(texts).toarray()\n","\n","        # Scale features (zero mean, unit variance)\n","        scaler = StandardScaler()\n","        X_train = scaler.fit_transform(X_train)\n","\n","        # Encode authors as integer labels\n","        le = LabelEncoder()\n","        y_train = le.fit_transform(authors)\n","\n","# ==========================================================\n","# Apply same models to the test set\n","# ==========================================================\n","for set, [authors, titles, texts] in data_dict.items():\n","    if set == 'test':  # key must be 'test'\n","        X_test = model.transform(texts).toarray()\n","        X_test = scaler.transform(X_test)\n","        test_titles = titles\n","\n","# --- Inspect processed vectors ---\n","\n","print(\"Training vectors shape:\", X_train.shape)\n","print(\"First 5 training vectors:\\n\", X_train[:5])\n","\n","print(\"\\nTest vectors shape:\", X_test.shape)\n","print(\"First 5 test vectors:\\n\", X_test[:5])\n","\n","# Optional: check feature names and order\n","print(\"\\nFirst 10 features in vectorizer:\", model.get_feature_names_out()[:10])\n"]},{"cell_type":"markdown","id":"599e2a61-9f27-442f-8c83-1d831ecbea7d","metadata":{"id":"599e2a61-9f27-442f-8c83-1d831ecbea7d"},"source":["## 4.1 Training a Classifier (by applying SVM)\n","\n","Especially in recent years, that have witnessed the rise of machine learning and computing power,  classification algorithms such as support vector machines (SVM’s, **support vector machines**) have become increasingly popular. A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that maximally separates data points of different classes in a high-dimensional space. SVM is effective in high-dimensional spaces and is versatile with different kernel functions for non-linear classification.\n","\n","For now, we will not occupy ourselves too much with the hyperparameters of SVM's just yet, and first take a look at some more general principles of training and evaluating classifiers.\n","\n","### 4.1.1 Preparing the Dataset for Training → `train_test_split`\n","\n","Above, we already declared a train and test set in separate folders. During training, however, it is considered good practice to hold out a **development set** (`X_dev`), also known as a **validation set**. It is a subset of the training set that is set aside during the training of a machine learning model. This heldout `X_dev` is not used in the training process but is instead used to evaluate the model's performance during development. The heldout dev set allows you to assess how well your model is likely to perform on unseen data (i.e. `X_test`), providing a better estimate of its generalization ability, offering a basis for tuning hyperparameters by evaluating different settings, detecting overfitting early to take preventive measures, and aiding in selecting the best performing model for production.\n","\n","Here is a list of the variables that you will encounter in the process of partitioning our data set:\n","\n","* `X_train` and `y_train`: The **full training set**: all vectorized text segments (`X_train`) labelled by authorship (`y_train`).\n","* `X_train_split` and `y_train_split`: The **remaining training set** after subtraction of the validation set.\n","* `X_dev` and `y_dev`: The **validation set**, subset of the training data  temporarily held out in order to function as a kind of stand-in test set.\n","* `X_test`:  The *actual* **test set**, i.e. texts unseen by the model, for which authorship are —truly, this time— unknown.\n","\n","Parameters to take into account when subtracting `X_dev` from `X_train` are the **split ratio** (`test_size=0.33`) and a **random seed** in the split process to ensure that the results are reproducible (`random_state`).\n","\n","### 4.1.2 Evaluation: Accuracy, Precision, Recall, F1 score\n","\n","Once our model is trained on `X_train_split` and known labels `y_train_split`, we are effectively able to test the model's quality by having it predict on the heldout `X_dev` set. This yields a vector of **predictions** `y_dev_pred`, which can be readily compared to what is commonly referred to as \"**ground truth**\" or \"**gold standard**\".\n","\n","During training, each of our authors (let's say for now, authors A, B, and C) is awarded a class label corresponding to a digit, e.g. `0`, `1`, `2`.\n","- The `y_dev`-array will, therefore, look like something like this: `[0, 0, 1, 1, 2, 2]`, where each of 3 authors in the training set is corresponded by 2 text samples in the development set.  \n","- Possibly, our model can output as prediction (`y_dev_pred`) the vector array `[0, 1, 1, 1, 2, 2]`.\n","\n","Clearly, it has made a mistake in misattributing the second text segment to class `1` (Author B) instead of class `0` (Author A).\n","\n","When comparing the golden standard against the predictions, we can extract several interesting evaluation metrics from a `classification_report`, yielding `accuracy`, `precision`, `recall`,`f1-score`.\n","\n","* `accuracy`: *\"How often did we correctly attribute the text segment to a given author?\"*  \n","  I.e. the percentage of correct predictions out of all the predictions made. The answer in our case may be obvious: 5 out of 6 times, 0.83.\n","* `precision`: *\"When we positively identified a text segment as written by a given author, how often was that true?\"*  \n","  Precision is calculated for each class separately and later averaged across classes. In our case, let us consider the example of Author B. In case of Authors A and C, the precision is in fact 100% in both cases: all positive identifications (1/1 for Author A and 2/2 for Author B) were indeed positive. In case of Author B, however, at one time `1` flared up where the outcome should have been `0` (= Author A). This is a **false positive**, and impairs our model's precision to 2/3 —out of 3 positive outcomes for Author B, only 2 were in fact correct—, and yields a score of 0.67. On average, our precision (when macro-averaged across all classes) is 0.89.\n","* `recall`: *\"Were we able to attribute all text segments of a given author to that author?\"*  \n","  Recall computes how many out of all predictions that should have been labelled positive were actually labelled such. Again, each class is first looked at individually. It turns out that, when we indeed look at the example of Author A, we in fact only caught half of the `0`'s we should have caught, because we falsely attributed an observation belonging to `0` to class `1`. The missed instance for class `0` is what we call a **false negative**, and impairs the recall for that class to 1/2, i.e. 0.5. For Authors B and C, the results are 2/2 and 2/2, both times 100%. Taken on average, then, when looking at our model's performance on all authors, the so-called macro recall adds up to 0.83.\n","* `f1-score`: A balanced score (harmonic mean) that combines precision and recall."]},{"cell_type":"code","execution_count":4,"id":"09daf3e9-bda5-4892-bc01-04adb97ec950","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09daf3e9-bda5-4892-bc01-04adb97ec950","executionInfo":{"status":"ok","timestamp":1758197919216,"user_tz":-120,"elapsed":90,"user":{"displayName":"Jeroen De Gussem UGent","userId":"05508689320763448270"}},"outputId":"05f75465-675d-4a5a-dad7-81360e843f26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensions of original data set:\n","(73, 250)\n","Dimensions of partitions (train and dev) set:\n","(48, 250)\n","(25, 250)\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        12\n","           1       1.00      1.00      1.00        10\n","           2       1.00      1.00      1.00         3\n","\n","    accuracy                           1.00        25\n","   macro avg       1.00      1.00      1.00        25\n","weighted avg       1.00      1.00      1.00        25\n","\n","\n","Predicted authorship:\n","                               Prediction\n","Scivias (2)_1       Hildegardis-Bingensis\n","Scivias (2)_2       Hildegardis-Bingensis\n","Scivias (2)_3   Bernardus-Claraevallensis\n","Scivias (2)_4       Hildegardis-Bingensis\n","Scivias (2)_5       Hildegardis-Bingensis\n","Scivias (2)_6       Hildegardis-Bingensis\n","Scivias (2)_7       Hildegardis-Bingensis\n","Scivias (2)_8       Hildegardis-Bingensis\n","Scivias (2)_9       Hildegardis-Bingensis\n","Scivias (2)_10      Hildegardis-Bingensis\n","Scivias (2)_11      Hildegardis-Bingensis\n","Scivias (2)_12      Hildegardis-Bingensis\n","Scivias (2)_13      Hildegardis-Bingensis\n","Scivias (2)_14      Hildegardis-Bingensis\n","Scivias (2)_15      Hildegardis-Bingensis\n","Scivias (2)_16      Hildegardis-Bingensis\n","Scivias (2)_17      Hildegardis-Bingensis\n","Scivias (2)_18      Hildegardis-Bingensis\n","Scivias (2)_19      Hildegardis-Bingensis\n","Scivias (2)_20      Hildegardis-Bingensis\n","Scivias (2)_21      Hildegardis-Bingensis\n","Scivias (2)_22      Hildegardis-Bingensis\n","Scivias (2)_23      Hildegardis-Bingensis\n","Scivias (2)_24      Hildegardis-Bingensis\n","Scivias (2)_25      Hildegardis-Bingensis\n","Scivias (2)_26      Hildegardis-Bingensis\n","Scivias (2)_27      Hildegardis-Bingensis\n","Scivias (2)_28      Hildegardis-Bingensis\n","Scivias (2)_29      Hildegardis-Bingensis\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","\n","\"\"\"\n","Train on partitioned X_train_split, y_train_split\n","Test on validation set X_dev => yields y_dev_pred\n","\"\"\"\n","\n","# Splits datasets into random train and test subsets.\n","X_train_split, X_dev, y_train_split, y_dev = train_test_split(X_train, y_train, test_size=0.33, random_state=1) # test_size: 1/3 of train data becomes validation set\n","\n","print('Dimensions of original data set:')\n","print(X_train.shape)\n","print('Dimensions of partitions (train and dev) set:')\n","print(X_train_split.shape)\n","print(X_dev.shape)\n","\n","# Initialize an SVM-classifier\n","svm_classifier = SVC(kernel='linear', C=1.0, random_state=42) # random seed ensures reproducibility\n","svm_classifier.fit(X_train_split, y_train_split)\n","\n","# Make predictions with model\n","y_dev_pred = svm_classifier.predict(X_dev) # y_pred = model predictions\n","\n","print(classification_report(y_dev, y_dev_pred)) # compare predictions to ground truth / gold standard\n","\n","\"\"\"\n","Test on test set\n","Yields y_pred (predictions) of authorship\n","\"\"\"\n","\n","y_pred = svm_classifier.predict(X_test)\n","predictions = le.inverse_transform(y_pred)\n","\n","print()\n","print(\"Predicted authorship:\")\n","\n","df = pd.DataFrame(predictions) # structures matrix X as a DataFrame\n","df.columns = ['Prediction'] # assigns column labels\n","df.index = test_titles\n","\n","print(df)"]},{"cell_type":"markdown","id":"54105e5a-bed2-4b56-b360-66bfdb5fee78","metadata":{"id":"54105e5a-bed2-4b56-b360-66bfdb5fee78"},"source":["## 4.2 `GridsearchCV()`: Tuning Parameters and Hyperparameters of the SVM (Advanced)\n","\n","In this section, we mainly repeat much of the above, but introduce the useful class `sklearn.model_selection.GridSearchCV`.\n","Think of what follows as a more advanced and specialized way of going about training your model. This time, we do not simply *choose* whatever parameters we think will work best, we statistically analyze and evaluate a series of varying presets, in order to gauge their performance on a more objective basis.\n","\n","An SVM has quite a few hyperparameters, such as the regularization parameter (`'C'`) and the kernel parameters (like `'linear'`).\n","Moreover, from a stylometric methodological perspective, we may want to experiment with varying feature types (function words, character n-grams, ...), feature vector lengths (`n_features`), and segment lengths (`sample_len`). Gridsearch can help us find the optimal settings, ensuring the SVM model achieves the highest possible accuracy and generalizes well to unseen data. This process helps to avoid the pitfalls of manual tuning (which can be subjective) and ensures a more robust and reliable model.\n","\n","Below, we first declare these various presets in containers, e.g. `sample_len_loop`, `feat_type_loop`, `feat_n_loop`, `c_options`, `kernel_options`, and `k_folds`.\n","\n","**READ FIRST: Searching many parameters at the same time can be quite costly and take a long time. Try to start your gridsearch by focussing on only a few of these parameters at a time, just so you can get acquainted with them.**"]},{"cell_type":"code","execution_count":5,"id":"bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22","executionInfo":{"status":"ok","timestamp":1758197947490,"user_tz":-120,"elapsed":23145,"user":{"displayName":"Jeroen De Gussem UGent","userId":"05508689320763448270"}},"outputId":"9aaf2609-ee0b-4ae1-d030-f153cd26bec8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Upload training files...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d71da872-adc2-4afb-b4b7-c0b025f20416\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d71da872-adc2-4afb-b4b7-c0b025f20416\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos_cleaned.txt to Petrus-Abaelardus_Commentariorum-super-S-Pauli-Epistolam-ad-Romanos_cleaned (9).txt\n","Saving Bernardus-Claraevallensis_Sermones-super-cantica-canticorum_cleaned.txt to Bernardus-Claraevallensis_Sermones-super-cantica-canticorum_cleaned (9).txt\n","Saving Hildegardis-Bingensis_Liber-divinorum-operum_cleaned.txt to Hildegardis-Bingensis_Liber-divinorum-operum_cleaned (9).txt\n"]}],"source":["from sklearn import svm\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import f1_score, make_scorer, recall_score, accuracy_score, precision_score\n","from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from string import punctuation\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import re\n","\n","# --- Upload training files ---\n","print(\"Upload training files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"train\")\n","\n","# -------------------------------\n","# PREPROCESS UPLOADED FILES ONCE\n","# -------------------------------\n","preprocessed_texts = {}  # filename -> list of words\n","authors_dict = {}        # filename -> author\n","titles_dict = {}         # filename -> title\n","\n","for filename in uploaded.keys():  # loop over uploaded files\n","    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]  # parse metadata\n","    text = open(filename, encoding='utf-8-sig').read().strip()             # read file\n","    words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()  # clean & tokenize\n","    preprocessed_texts[filename] = words\n","    authors_dict[filename] = author\n","    titles_dict[filename] = title"]},{"cell_type":"markdown","id":"8e234161-8611-4682-bb15-dbfdfa2c602c","metadata":{"id":"8e234161-8611-4682-bb15-dbfdfa2c602c"},"source":["Consequently, by using `GridSearchCV()`, one can efficiently navigate through the classifier's parameter space. Moreover, we expand its functionality in the code to follow by introducing a number of implementations particularly suitable for non-traditional authorship attribution. The code block below is an example.\n","\n","1. **Preprocessing**: First, we open and preprocess our files again (a step you are familiar with by now), and store our `texts` (text segments by a certain `sample_len`) in a variable `X_train`. Consequently, we label the segments by authorship and store the labels in `y_train` (if there are three authors, the labels should be `0` for author A, `1` for author B, and `2` for author C).\n","2. **Vectorization**: We initialize various vectorization options, where we take into account the feature type (`word` or `char`), declare our `n_gram` preferences, and decide whether we want to input raw frequencies (`CountVectorizer()`) or else TF-IDF frequencies (`TfidfVectorizer`).\n","3. **Pipeline and Parameter Grid**: We build a `pipe` (`Pipeline`) a tool for chaining together these data preprocessing steps and machine learning algorithms into a single object. This enables seamless and efficient handling of data transformation and model training, facilitating the creation of end-to-end machine learning workflows. We store our variables in a dictionary `param_grid`, specifying the hyperparameter values to search over during the grid search. It allows for systematic exploration of different combinations of hyperparameters to identify the optimal configuration for a machine learning model.\n","4. **Grid Search and Cross-Validated Results**: Finally, we introduce a new important concept, that of cross-Validation (CV). In fact, CV is an advanced and more reliable way of going about `train_test_split` as it was introduced in the block of code earlier. With CV, the dataset is divided into *k* equal-sized **folds**. Consequently, the model is trained *k* times, where each time it is trained on *k*−1 folds (which corresponds to `X_train_split` above) and tested on the remaining fold (`X_dev`). The performance metrics (accuracy, precision, recall, f1-score) are averaged over the *k* trials to give a more reliable estimate of the model's performance (information you can extract from `results['mean_test_accuracy_score']`).\n","\n","This helps ensure that the model is not overly dependent on a particular subset of the data, as well as provides a more accurate estimate of the model’s performance on unseen data.\n","\n","Try to tweak the various parameter settings above, and then run the code below.\n","\n","**Searching many parameters at the same time can be quite costly and take a long time. Try to start your gridsearch by focussing on only a few of these parameters at a time, just so you can get acquainted with them.**"]},{"cell_type":"code","execution_count":8,"id":"29ff0e2a-5722-4241-bf40-38086f36da0b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622,"referenced_widgets":["dbc090c9a8d84b9a8ff95255fe6b6147","5ba9426aa11044aca159230185a1376c","d77421d488ac49bb9e99a6708666d25e","2d75af004be2438680537df087e5e7b8","eda7423a997b443fb84cd9599ec37329","c92027425a684fbe832c0d94ae1ce4dc","efb18948b7cc46818f0c28361b7c35e5","8e766b02a207483f8f4f9521088cc0ca","1d066115440044cf97fecd04a2f13d80","5e49e2f291f24afe93712de722d7e6e4","47bbaacae2594cd0b6957b627e5b20d6"]},"id":"29ff0e2a-5722-4241-bf40-38086f36da0b","executionInfo":{"status":"ok","timestamp":1758198111133,"user_tz":-120,"elapsed":73963,"user":{"displayName":"Jeroen De Gussem UGent","userId":"05508689320763448270"}},"outputId":"1fa3fa8a-2ec5-4ea5-8342-9225100d9f1e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Sample length:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc090c9a8d84b9a8ff95255fe6b6147"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["                           model  accuracy  precision    recall  f1 score\n","31     tfidf_MFW-600-3000-10-rbf  1.000000   1.000000  1.000000  1.000000\n","30  tfidf_MFW-600-3000-10-linear  1.000000   1.000000  1.000000  1.000000\n","29      tfidf_MFW-600-3000-1-rbf  1.000000   1.000000  1.000000  1.000000\n","28   tfidf_MFW-600-3000-1-linear  1.000000   1.000000  1.000000  1.000000\n","27     tfidf_MFW-200-3000-10-rbf  1.000000   1.000000  1.000000  1.000000\n","26  tfidf_MFW-200-3000-10-linear  1.000000   1.000000  1.000000  1.000000\n","25      tfidf_MFW-200-3000-1-rbf  1.000000   1.000000  1.000000  1.000000\n","24   tfidf_MFW-200-3000-1-linear  1.000000   1.000000  1.000000  1.000000\n","23       raw_MFW-600-3000-10-rbf  1.000000   1.000000  1.000000  1.000000\n","22    raw_MFW-600-3000-10-linear  1.000000   1.000000  1.000000  1.000000\n","21        raw_MFW-600-3000-1-rbf  1.000000   1.000000  1.000000  1.000000\n","20     raw_MFW-600-3000-1-linear  1.000000   1.000000  1.000000  1.000000\n","19       raw_MFW-200-3000-10-rbf  1.000000   1.000000  1.000000  1.000000\n","18    raw_MFW-200-3000-10-linear  1.000000   1.000000  1.000000  1.000000\n","17        raw_MFW-200-3000-1-rbf  1.000000   1.000000  1.000000  1.000000\n","16     raw_MFW-200-3000-1-linear  1.000000   1.000000  1.000000  1.000000\n","14   tfidf_MFW-600-500-10-linear  0.994629   0.996084  0.992472  0.994232\n","12    tfidf_MFW-600-500-1-linear  0.994629   0.996084  0.992472  0.994232\n","4       raw_MFW-600-500-1-linear  0.994629   0.996084  0.992472  0.994232\n","6      raw_MFW-600-500-10-linear  0.994629   0.996084  0.992472  0.994232\n","3         raw_MFW-200-500-10-rbf  0.991952   0.994203  0.988357  0.991110\n","0       raw_MFW-200-500-1-linear  0.989264   0.991128  0.986010  0.988433\n","2      raw_MFW-200-500-10-linear  0.989264   0.991128  0.986010  0.988433\n","1          raw_MFW-200-500-1-rbf  0.989264   0.992287  0.984944  0.988376\n","11      tfidf_MFW-200-500-10-rbf  0.989264   0.992287  0.984944  0.988376\n","7         raw_MFW-600-500-10-rbf  0.987914   0.991353  0.982887  0.986868\n","9        tfidf_MFW-200-500-1-rbf  0.987919   0.991370  0.982887  0.986800\n","5          raw_MFW-600-500-1-rbf  0.986570   0.990404  0.981532  0.985701\n","15      tfidf_MFW-600-500-10-rbf  0.986570   0.990404  0.981532  0.985701\n","13       tfidf_MFW-600-500-1-rbf  0.985226   0.989471  0.979474  0.984142\n","8     tfidf_MFW-200-500-1-linear  0.982548   0.984740  0.976787  0.980517\n","10   tfidf_MFW-200-500-10-linear  0.982548   0.984740  0.976787  0.980517\n"]}],"source":["# -------------------------------\n","# PARAMETERS\n","# -------------------------------\n","sample_len_loop = [500, 3000]         # test sample lengths\n","feat_type_loop = ['raw_MFW','tfidf_MFW']  # smaller subset for testing\n","feat_n_loop = [200, 600]              # number of features\n","c_options = [1, 10]                   # SVM C values\n","kernel_options = ['linear', 'rbf']    # SVM kernels, try 'poly' and 'sigmoid' too\n","k_folds = [3]                         # cross-validation folds\n","n_iter_random = 4                      # number of random hyperparameter combos\n","\n","vectorizer_params = {\n","    'raw_MFW': (CountVectorizer, 'word', (1,1)),\n","    'tfidf_MFW': (TfidfVectorizer, 'word', (1,1)),\n","    'raw_4grams': (CountVectorizer, 'char', (4,4)),\n","    'tfidf_4grams': (TfidfVectorizer, 'char', (4,4))\n","}\n","\n","# -------------------------------\n","# CONTAINERS FOR RESULTS\n","# -------------------------------\n","all_grid_scores = []          # store aggregated CV metrics per parameter combo\n","all_parameter_combos = []     # store corresponding parameter settings\n","\n","# Use a stratified CV object to ensure balanced folds across classes\n","cv = StratifiedKFold(n_splits=k_folds[0], shuffle=True, random_state=42)\n","\n","# --- Outer loop over sample lengths ---\n","for sample_len in tqdm(sample_len_loop, desc=\"Sample length\"):\n","    # -------------------------------\n","    # SLICE PREPROCESSED TEXTS INTO SAMPLES (chunk texts once per sample_len)\n","    # -------------------------------\n","    texts, authors, titles = [], [], []\n","    for filename, words in preprocessed_texts.items():\n","        # split the text into consecutive chunks of length `sample_len`\n","        bulk = [words[i:i+sample_len] for i in range(0, len(words), sample_len)]\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:  # skip last chunk if shorter than sample_len\n","                texts.append(\" \".join(sample))          # reconstruct sample as string\n","                authors.append(authors_dict[filename])  # append corresponding author\n","                titles.append(f\"{titles_dict[filename]}_{index+1}\")  # unique title per chunk\n","\n","    # Encode labels for classifier\n","    y_train = LabelEncoder().fit_transform(authors)\n","\n","    # Ensure each class has at least as many samples as CV folds\n","    _, class_counts = np.unique(y_train, return_counts=True)\n","    if class_counts.min() < k_folds[0]:\n","        print(f\"[SKIP] sample_len={sample_len}: min class count {class_counts.min()} < cv={k_folds[0]}.\")\n","        continue\n","\n","    # --- Loop over feature extraction configurations ---\n","    for feat_type in feat_type_loop:\n","        for n_feats in feat_n_loop:\n","            # -------------------------------\n","            # INITIALIZE AND FIT VECTORIZER (specific to feat_type & n_feats)\n","            # -------------------------------\n","            vectorizer_class, analyzer, ngram_range = vectorizer_params[feat_type]\n","            vectorizer = vectorizer_class(\n","                analyzer=analyzer,\n","                ngram_range=ngram_range,\n","                max_features=n_feats,\n","                lowercase=False,     # texts already pre-lowercased\n","                dtype=np.float32     # reduces memory usage & speeds up computation\n","            )\n","            X = vectorizer.fit_transform(texts)   # sparse matrix representation\n","\n","            # -------------------------------\n","            # DEFINE PIPELINE (scaler + classifier)\n","            # -------------------------------\n","            pipe = Pipeline([\n","                ('scaler', StandardScaler(with_mean=False)),          # preserves sparse format\n","                ('classifier', svm.SVC(probability=True))            # SVM classifier\n","            ])\n","\n","            # -------------------------------\n","            # RANDOMIZED SEARCH CONFIGURATION\n","            # -------------------------------\n","            param_grid = {\n","                'classifier__C': c_options,\n","                'classifier__kernel': kernel_options\n","            }\n","\n","            # define scoring metrics for evaluation\n","            scoring = {\n","                'accuracy_score': make_scorer(accuracy_score),\n","                'precision_score': make_scorer(precision_score, average='macro', zero_division=0),\n","                'recall_score': make_scorer(recall_score, average='macro', zero_division=0),\n","                'f1_score': make_scorer(f1_score, average='macro', zero_division=0)\n","            }\n","\n","            # setup RandomizedSearchCV\n","            grid = RandomizedSearchCV(\n","                pipe,\n","                param_distributions=param_grid,\n","                n_iter=n_iter_random,               # number of random combos to try\n","                cv=cv,                               # stratified CV\n","                n_jobs=-1,                           # use all CPUs\n","                scoring=scoring,                     # multiple metrics\n","                refit='f1_score',              # best model selected by accuracy\n","                verbose=0,\n","                error_score=np.nan                   # assign NaN on failure, avoid tracebacks\n","            )\n","\n","            # -------------------------------\n","            # FIT MODEL ON TRAINING DATA\n","            # -------------------------------\n","            grid.fit(X, y_train)\n","            results = grid.cv_results_\n","\n","            # -------------------------------\n","            # STORE CV RESULTS\n","            # -------------------------------\n","            for idx, params in enumerate(results['params']):\n","                # store mean scores for current parameter combination\n","                all_grid_scores.append((\n","                    results['mean_test_accuracy_score'][idx],\n","                    results['mean_test_precision_score'][idx],\n","                    results['mean_test_recall_score'][idx],\n","                    results['mean_test_f1_score'][idx]\n","                ))\n","                # store corresponding parameter settings\n","                all_parameter_combos.append((\n","                    feat_type, n_feats, sample_len,\n","                    params['classifier__C'], params['classifier__kernel']\n","                ))\n","\n","# -------------------------------\n","# CREATE REPORT DATAFRAME\n","# -------------------------------\n","import pandas as pd\n","full_report = []\n","for (acc, prec, rec, f1), params in zip(all_grid_scores, all_parameter_combos):\n","    model_name = '-'.join([str(i) for i in params])   # create unique model name from parameters\n","    full_report.append((model_name, acc, prec, rec, f1))\n","\n","# convert to DataFrame and sort by f1 score\n","df = pd.DataFrame(full_report, columns=['model', 'accuracy', 'precision', 'recall', 'f1 score'])\n","df_sorted = df.sort_values(by='f1 score', ascending=False)\n","print(df_sorted)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"dbc090c9a8d84b9a8ff95255fe6b6147":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ba9426aa11044aca159230185a1376c","IPY_MODEL_d77421d488ac49bb9e99a6708666d25e","IPY_MODEL_2d75af004be2438680537df087e5e7b8"],"layout":"IPY_MODEL_eda7423a997b443fb84cd9599ec37329"}},"5ba9426aa11044aca159230185a1376c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92027425a684fbe832c0d94ae1ce4dc","placeholder":"​","style":"IPY_MODEL_efb18948b7cc46818f0c28361b7c35e5","value":"Sample length: 100%"}},"d77421d488ac49bb9e99a6708666d25e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e766b02a207483f8f4f9521088cc0ca","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d066115440044cf97fecd04a2f13d80","value":2}},"2d75af004be2438680537df087e5e7b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e49e2f291f24afe93712de722d7e6e4","placeholder":"​","style":"IPY_MODEL_47bbaacae2594cd0b6957b627e5b20d6","value":" 2/2 [01:13&lt;00:00, 31.48s/it]"}},"eda7423a997b443fb84cd9599ec37329":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92027425a684fbe832c0d94ae1ce4dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb18948b7cc46818f0c28361b7c35e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e766b02a207483f8f4f9521088cc0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d066115440044cf97fecd04a2f13d80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e49e2f291f24afe93712de722d7e6e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47bbaacae2594cd0b6957b627e5b20d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}