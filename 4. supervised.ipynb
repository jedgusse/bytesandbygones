{"cells":[{"cell_type":"markdown","id":"268ecd03-c139-4090-a3c4-90526a2dde66","metadata":{"id":"268ecd03-c139-4090-a3c4-90526a2dde66"},"source":["# 4. Supervised Analysis and Classifiers\n","\n","Supervised algorithms (or **supervised** models) observe the way in which the feature inputs `X` (e.g. function word frequencies of known texts) correlate with class outputs `y` (e.g. authorship / label of the text). A supervised model ‘observes’ correctly labelled, preclassified X-y pairs, in order to register meaningful correlations (e.g. between certain function word patterns X with certain authors y). This process is called ‘training,’ and the X-y pairs which the model trains on correspond to what is commonly referred to as `training data`. Consequently, once this learning process has taken place, the model can be confronted with `test data,` comprising previously unobserved and unclassified texts. On the basis of what it has observed, the supervised model can make a prediction (classification), and assign the unseen test data to a class, either by a hard decision or by outputting a probability score.\n","\n","Classification is a considerable field of research on its own. There are **many types** of classifiers, and it is not always clear which one will perform best, and why. Varying types of classifiers tend to react differently to different problems, have a variety of parametrization options and require other methods by which to optimize their performance during training. A big advantage of supervised machine-learning methods, ‘text classification’ (Sebastiani 2002), is the **possibility of evaluation**. By making different combinations of parameters, such as the feature set, the vector length (number of features), sample length, vectorization method, scaling method, etc., and evaluating how well they can be fitted to a class (author), scholars can finetune and optimize these parameters.\n","Before we proceed, we **repeat**, with the block of code below, some of **the steps from the previous notebook**.\n","\n","These are:\n","\n","1. Loading and segmentation of documents, containers `authors`, `titles`, `texts`\n","2. Vectorization of `texts` to matrix `X` containing vectors for all text segments\n","3. Scale `X` by applying `StandardScaler()`\n","\n","Note that, as opposed to the three previous notebooks, we are now introducing a **test corpus** of allegedly **unknown authorship**. These files can be found in our `'corpus/test/'` folder. Below, we will apply a classifier to attribute the text to one of the **known classes**, i.e. our training set from the `'corpus/train/'` folder."]},{"cell_type":"code","execution_count":null,"id":"b7fa8ceb-1f6f-4bff-b015-2ed6eae05410","metadata":{"id":"b7fa8ceb-1f6f-4bff-b015-2ed6eae05410"},"outputs":[],"source":["from google.colab import files\n","from string import punctuation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# --- Parameters ---\n","sample_len = 5000  # length of each text segment in words\n","data_dict = {\"train\": [[], [], []], \"test\": [[], [], []]}\n","\n","def process_uploaded(uploaded, dict_key):\n","    \"\"\"Helper: clean, segment, and add uploaded texts to data_dict.\"\"\"\n","    authors, titles, texts = data_dict[dict_key]\n","\n","    for filepath, fileobj in uploaded.items():\n","        filename = filepath.split(\"/\")[-1]  # e.g. \"author_title.txt\"\n","\n","        # Extract author and title from filename\n","        author = filename.split(\".\")[0].split(\"_\")[0]\n","        title = filename.split(\".\")[0].split(\"_\")[1]\n","\n","        # Decode uploaded bytes\n","        text = fileobj.decode(\"utf-8-sig\")\n","\n","        # Tokenize and clean\n","        bulk = []\n","        for word in text.strip().split():\n","            word = re.sub(r'\\d+', '', word)  # remove digits\n","            word = re.sub('[%s]' % re.escape(punctuation), '', word)  # remove punctuation\n","            word = word.lower()\n","            if word != \"\":\n","                bulk.append(word)\n","\n","        # Split text into equal-length samples\n","        bulk = [bulk[i:i+sample_len] for i in range(0, len(bulk), sample_len)]\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:\n","                authors.append(author)\n","                titles.append(title + \"_{}\".format(index + 1))\n","                texts.append(\" \".join(sample))\n","\n","\n","# --- Phase 1: Upload training files ---\n","print(\"Upload training files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"train\")\n","\n","# --- Phase 2: Upload test files ---\n","print(\"Upload test files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"test\")\n","\n","# After both phases:\n","# data_dict[\"train\"] = [authors, titles, texts]\n","# data_dict[\"test\"]  = [authors, titles, texts]"]},{"cell_type":"code","execution_count":null,"id":"b7976688-fa6a-45da-8d77-e16fb7883b05","metadata":{"id":"b7976688-fa6a-45da-8d77-e16fb7883b05"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","\n","# ==========================================================\n","# Vectorize and scale the training and test sets\n","# ==========================================================\n","\n","for set, [authors, titles, texts] in data_dict.items():\n","    if set == 'train':  # key must be 'train'\n","        # Vectorize training texts (bag-of-words, top 250 unigrams)\n","        model = CountVectorizer(\n","            max_features=250,\n","            analyzer='word',\n","            ngram_range=(1, 1)\n","        )\n","        X_train = model.fit_transform(texts).toarray()\n","\n","        # Sort features by frequency for consistent ordering\n","        feat_frequencies = np.asarray(X_train.sum(axis=0)).flatten()\n","        features = model.get_feature_names_out()\n","        feat_freq_df = pd.DataFrame({'feature': features, 'frequency': feat_frequencies})\n","        feat_freq_df = feat_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n","        sorted_features = feat_freq_df['feature'].tolist()\n","        sorted_indices = [model.vocabulary_[feat] for feat in sorted_features]\n","        X_train_sorted = X_train[:, sorted_indices]\n","\n","        # Refit vectorizer with sorted vocabulary\n","        model = CountVectorizer(\n","            stop_words=[],\n","            analyzer='word',\n","            vocabulary=sorted_features,\n","            ngram_range=(1, 1)\n","        )\n","        X_train = model.fit_transform(texts).toarray()\n","\n","        # Scale features (zero mean, unit variance)\n","        scaler = StandardScaler()\n","        X_train = scaler.fit_transform(X_train)\n","\n","        # Encode authors as integer labels\n","        le = LabelEncoder()\n","        y_train = le.fit_transform(authors)\n","\n","# ==========================================================\n","# Apply same models to the test set\n","# ==========================================================\n","for set, [authors, titles, texts] in data_dict.items():\n","    if set == 'test':  # key must be 'test'\n","        X_test = model.transform(texts).toarray()\n","        X_test = scaler.transform(X_test)\n","        test_titles = titles\n","\n","# --- Inspect processed vectors ---\n","\n","print(\"Training vectors shape:\", X_train.shape)\n","print(\"First 5 training vectors:\\n\", X_train[:5])\n","\n","print(\"\\nTest vectors shape:\", X_test.shape)\n","print(\"First 5 test vectors:\\n\", X_test[:5])\n","\n","# Optional: check feature names and order\n","print(\"\\nFirst 10 features in vectorizer:\", model.get_feature_names_out()[:10])\n"]},{"cell_type":"markdown","id":"599e2a61-9f27-442f-8c83-1d831ecbea7d","metadata":{"id":"599e2a61-9f27-442f-8c83-1d831ecbea7d"},"source":["## 4.1 Training a Classifier (by applying SVM)\n","\n","Especially in recent years, that have witnessed the rise of machine learning and computing power,  classification algorithms such as support vector machines (SVM’s, **support vector machines**) have become increasingly popular. A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that maximally separates data points of different classes in a high-dimensional space. SVM is effective in high-dimensional spaces and is versatile with different kernel functions for non-linear classification.\n","\n","For now, we will not occupy ourselves too much with the hyperparameters of SVM's just yet, and first take a look at some more general principles of training and evaluating classifiers.\n","\n","### 4.1.1 Preparing the Dataset for Training → `train_test_split`\n","\n","Above, we already declared a train and test set in separate folders. During training, however, it is considered good practice to hold out a **development set** (`X_dev`), also known as a **validation set**. It is a subset of the training set that is set aside during the training of a machine learning model. This heldout `X_dev` is not used in the training process but is instead used to evaluate the model's performance during development. The heldout dev set allows you to assess how well your model is likely to perform on unseen data (i.e. `X_test`), providing a better estimate of its generalization ability, offering a basis for tuning hyperparameters by evaluating different settings, detecting overfitting early to take preventive measures, and aiding in selecting the best performing model for production.\n","\n","Here is a list of the variables that you will encounter in the process of partitioning our data set:\n","\n","* `X_train` and `y_train`: The **full training set**: all vectorized text segments (`X_train`) labelled by authorship (`y_train`).\n","* `X_train_split` and `y_train_split`: The **remaining training set** after subtraction of the validation set.\n","* `X_dev` and `y_dev`: The **validation set**, subset of the training data  temporarily held out in order to function as a kind of stand-in test set.\n","* `X_test`:  The *actual* **test set**, i.e. texts unseen by the model, for which authorship are —truly, this time— unknown.\n","\n","Parameters to take into account when subtracting `X_dev` from `X_train` are the **split ratio** (`test_size=0.33`) and a **random seed** in the split process to ensure that the results are reproducible (`random_state`).\n","\n","### 4.1.2 Evaluation: Accuracy, Precision, Recall, F1 score\n","\n","Once our model is trained on `X_train_split` and known labels `y_train_split`, we are effectively able to test the model's quality by having it predict on the heldout `X_dev` set. This yields a vector of **predictions** `y_dev_pred`, which can be readily compared to what is commonly referred to as \"**ground truth**\" or \"**gold standard**\".\n","\n","During training, each of our authors (let's say for now, authors A, B, and C) is awarded a class label corresponding to a digit, e.g. `0`, `1`, `2`.\n","- The `y_dev`-array will, therefore, look like something like this: `[0, 0, 1, 1, 2, 2]`, where each of 3 authors in the training set is corresponded by 2 text samples in the development set.  \n","- Possibly, our model can output as prediction (`y_dev_pred`) the vector array `[0, 1, 1, 1, 2, 2]`.\n","\n","Clearly, it has made a mistake in misattributing the second text segment to class `1` (Author B) instead of class `0` (Author A).\n","\n","When comparing the golden standard against the predictions, we can extract several interesting evaluation metrics from a `classification_report`, yielding `accuracy`, `precision`, `recall`,`f1-score`.\n","\n","* `accuracy`: *\"How often did we correctly attribute the text segment to a given author?\"*  \n","  I.e. the percentage of correct predictions out of all the predictions made. The answer in our case may be obvious: 5 out of 6 times, 0.83.\n","* `precision`: *\"When we positively identified a text segment as written by a given author, how often was that true?\"*  \n","  Precision is calculated for each class separately and later averaged across classes. In our case, let us consider the example of Author B. In case of Authors A and C, the precision is in fact 100% in both cases: all positive identifications (1/1 for Author A and 2/2 for Author B) were indeed positive. In case of Author B, however, at one time `1` flared up where the outcome should have been `0` (= Author A). This is a **false positive**, and impairs our model's precision to 2/3 —out of 3 positive outcomes for Author B, only 2 were in fact correct—, and yields a score of 0.67. On average, our precision (when macro-averaged across all classes) is 0.89.\n","* `recall`: *\"Were we able to attribute all text segments of a given author to that author?\"*  \n","  Recall computes how many out of all predictions that should have been labelled positive were actually labelled such. Again, each class is first looked at individually. It turns out that, when we indeed look at the example of Author A, we in fact only caught half of the `0`'s we should have caught, because we falsely attributed an observation belonging to `0` to class `1`. The missed instance for class `0` is what we call a **false negative**, and impairs the recall for that class to 1/2, i.e. 0.5. For Authors B and C, the results are 2/2 and 2/2, both times 100%. Taken on average, then, when looking at our model's performance on all authors, the so-called macro recall adds up to 0.83.\n","* `f1-score`: A balanced score (harmonic mean) that combines precision and recall."]},{"cell_type":"code","execution_count":null,"id":"09daf3e9-bda5-4892-bc01-04adb97ec950","metadata":{"id":"09daf3e9-bda5-4892-bc01-04adb97ec950"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","\n","\"\"\"\n","Train on partitioned X_train_split, y_train_split\n","Test on validation set X_dev => yields y_dev_pred\n","\"\"\"\n","\n","# Split X_train into a smaller training set and a validation (dev) set\n","X_train_split, X_dev, y_train_split, y_dev = train_test_split(\n","    X_train, y_train, test_size=0.33, random_state=1\n",")  # test_size=0.33 means 1/3 of the data is used for validation\n","\n","# Show dataset dimensions before and after splitting\n","print('Dimensions of original data set:')\n","print(X_train.shape)\n","print('Dimensions of partitions (train and dev) set:')\n","print(X_train_split.shape)\n","print(X_dev.shape)\n","\n","# === Visualization of the split process ===\n","fig, ax = plt.subplots(figsize=(8, 3))\n","\n","# Box for original dataset\n","ax.add_patch(Rectangle((0.1, 0.4), 0.3, 0.3, facecolor='lightgray'))\n","ax.text(0.25, 0.55, f'Original Data\\n{X_train.shape[0]} samples', ha='center')\n","\n","# Box for train split\n","ax.add_patch(Rectangle((0.55, 0.6), 0.3, 0.2, facecolor='steelblue'))\n","ax.text(0.7, 0.7, f'Train Split\\n{X_train_split.shape[0]} samples',\n","        ha='center', color='white')\n","\n","# Box for dev split\n","ax.add_patch(Rectangle((0.55, 0.3), 0.3, 0.2, facecolor='orange'))\n","ax.text(0.7, 0.4, f'Dev Split\\n{X_dev.shape[0]} samples',\n","        ha='center', color='black')\n","\n","# Arrows from original → train/dev\n","ax.annotate('', xy=(0.55, 0.7), xytext=(0.4, 0.55),\n","            arrowprops=dict(arrowstyle='->'))\n","ax.annotate('', xy=(0.55, 0.4), xytext=(0.4, 0.55),\n","            arrowprops=dict(arrowstyle='->'))\n","\n","ax.axis('off')\n","plt.title('Train/Test Split Visualization')\n","plt.show()\n","\n","# === Train and evaluate an SVM classifier ===\n","svm_classifier = SVC(kernel='sigmoid', C=1, random_state=50)  # sigmoid kernel SVM\n","svm_classifier.fit(X_train_split, y_train_split)  # train on training split\n","\n","# Predict labels on validation set\n","y_dev_pred = svm_classifier.predict(X_dev)\n","\n","# Print evaluation metrics for validation performance\n","print(classification_report(y_dev, y_dev_pred))\n","\n","\"\"\"\n","Test on final test set\n","Yields y_pred (predictions) of authorship\n","\"\"\"\n","\n","# Predict authorship on the true test set (unseen data)\n","y_pred = svm_classifier.predict(X_test)\n","predictions = le.inverse_transform(y_pred)  # convert encoded labels back to names\n","\n","print()\n","print(\"Predicted authorship:\")\n","\n","# Store predictions in a DataFrame for readability\n","df = pd.DataFrame(predictions)\n","df.columns = ['Prediction']\n","df.index = test_titles  # assign test document titles as row labels\n","\n","print(df)"]},{"cell_type":"markdown","id":"54105e5a-bed2-4b56-b360-66bfdb5fee78","metadata":{"id":"54105e5a-bed2-4b56-b360-66bfdb5fee78"},"source":["## 4.2 Gridsearching: Tuning Parameters and Hyperparameters of your SVM classifier (Advanced)\n","\n","In this section, we mainly repeat much of the above, but introduce a new useful idea: gridsearching.\n","Think of what follows as a more advanced and specialized way of going about training your model. This time, we do not simply *choose* whatever parameters we think will work best, we statistically analyze and evaluate a series of varying presets, in order to gauge their performance on a more objective basis.\n","\n","An SVM has quite a few hyperparameters, such as the regularization parameter (`'C'`) and the kernel parameters (like `'linear'`).\n","Moreover, from a stylometric methodological perspective, we may want to experiment with varying feature types (function words, character n-grams, ...), feature vector lengths (`n_features`), and segment lengths (`sample_len`). Gridsearch can help us find the optimal settings, ensuring the SVM model achieves the highest possible accuracy and generalizes well to unseen data. This process helps to avoid the pitfalls of manual tuning (which can be subjective) and ensures a more robust and reliable model.\n","\n","Below, we first declare these various presets in containers, e.g. `sample_len_loop`, `feat_type_loop`, `feat_n_loop`, `c_options`, `kernel_options`, and `k_folds`."]},{"cell_type":"code","execution_count":null,"id":"bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22","metadata":{"id":"bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22"},"outputs":[],"source":["from sklearn import svm\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import f1_score, make_scorer, recall_score, accuracy_score, precision_score\n","from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from string import punctuation\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","import numpy as np\n","import re\n","\n","# --- Upload training files ---\n","print(\"Upload training files...\")\n","uploaded = files.upload()\n","process_uploaded(uploaded, \"train\")\n","\n","# -------------------------------\n","# PREPROCESS UPLOADED FILES ONCE\n","# -------------------------------\n","preprocessed_texts = {}  # filename -> list of words\n","authors_dict = {}        # filename -> author\n","titles_dict = {}         # filename -> title\n","\n","for filename in uploaded.keys():  # loop over uploaded files\n","    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]  # parse metadata\n","    text = open(filename, encoding='utf-8-sig').read().strip()             # read file\n","    words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()  # clean & tokenize\n","    preprocessed_texts[filename] = words\n","    authors_dict[filename] = author\n","    titles_dict[filename] = title"]},{"cell_type":"markdown","id":"8e234161-8611-4682-bb15-dbfdfa2c602c","metadata":{"id":"8e234161-8611-4682-bb15-dbfdfa2c602c"},"source":["Below we systematically vary several SVM **parameters** and evaluate performance with **cross-validation**:\n","\n","- **Sample lengths (`sample_len_loop`)** - how many words go into each text segment (e.g. 500 vs 3000).  \n","- **Feature types (`feat_type_loop`)** - raw vs. TF-IDF word or character n-gram frequencies.  \n","- **Number of features (`feat_n_loop`)** - size of the vocabulary included in the model, e.g. 200 or 600 most frequent features.\n","- **SVM hyperparameters (`C`, `kernel`)** - controlling margin hardness and linear vs. non-linear decision boundaries.\n","\n","The code proceeds in **nested loops**:  \n","1. Slice texts into fixed-length chunks.  \n","2. Encode author labels for supervised learning.  \n","3. Extract features using the chosen vectorizer.  \n","4. Train SVM classifiers inside a **pipeline** (scaler + classifier).  \n","5. Use `RandomizedSearchCV` to try different parameter combinations with stratified cross-validation.  \n","6. Collect evaluation scores (accuracy, precision, recall, F1) for each setup.  \n","\n","Finally, results are aggregated in a **DataFrame** that lists each model configuration with its mean cross-validation scores.  \n","The table is sorted by F1 score to highlight the best-performing setup."]},{"cell_type":"code","execution_count":null,"id":"29ff0e2a-5722-4241-bf40-38086f36da0b","metadata":{"id":"29ff0e2a-5722-4241-bf40-38086f36da0b"},"outputs":[],"source":["# -------------------------------\n","# PARAMETERS\n","# -------------------------------\n","sample_len_loop = [500, 3000]         # test sample lengths\n","feat_type_loop = ['raw_MFW','tfidf_MFW']  # smaller subset for testing\n","feat_n_loop = [200, 600]              # number of features\n","c_options = [1, 10]                   # SVM C values\n","kernel_options = ['linear', 'rbf']    # SVM kernels, try 'poly' and 'sigmoid' too\n","k_folds = [3]                         # cross-validation folds\n","n_iter_random = 4                      # number of random hyperparameter combos\n","\n","vectorizer_params = {\n","    'raw_MFW': (CountVectorizer, 'word', (1,1)),\n","    'tfidf_MFW': (TfidfVectorizer, 'word', (1,1)),\n","    'raw_4grams': (CountVectorizer, 'char', (4,4)),\n","    'tfidf_4grams': (TfidfVectorizer, 'char', (4,4))\n","}\n","\n","# -------------------------------\n","# CONTAINERS FOR RESULTS\n","# -------------------------------\n","all_grid_scores = []          # store aggregated CV metrics per parameter combo\n","all_parameter_combos = []     # store corresponding parameter settings\n","\n","# Use a stratified CV object to ensure balanced folds across classes\n","cv = StratifiedKFold(n_splits=k_folds[0], shuffle=True, random_state=42)\n","\n","# --- Outer loop over sample lengths ---\n","for sample_len in tqdm(sample_len_loop, desc=\"Sample length\"):\n","    # -------------------------------\n","    # SLICE PREPROCESSED TEXTS INTO SAMPLES (chunk texts once per sample_len)\n","    # -------------------------------\n","    texts, authors, titles = [], [], []\n","    for filename, words in preprocessed_texts.items():\n","        # split the text into consecutive chunks of length `sample_len`\n","        bulk = [words[i:i+sample_len] for i in range(0, len(words), sample_len)]\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:  # skip last chunk if shorter than sample_len\n","                texts.append(\" \".join(sample))          # reconstruct sample as string\n","                authors.append(authors_dict[filename])  # append corresponding author\n","                titles.append(f\"{titles_dict[filename]}_{index+1}\")  # unique title per chunk\n","\n","    # Encode labels for classifier\n","    y_train = LabelEncoder().fit_transform(authors)\n","\n","    # Ensure each class has at least as many samples as CV folds\n","    _, class_counts = np.unique(y_train, return_counts=True)\n","    if class_counts.min() < k_folds[0]:\n","        print(f\"[SKIP] sample_len={sample_len}: min class count {class_counts.min()} < cv={k_folds[0]}.\")\n","        continue\n","\n","    # --- Loop over feature extraction configurations ---\n","    for feat_type in feat_type_loop:\n","        for n_feats in feat_n_loop:\n","            # -------------------------------\n","            # INITIALIZE AND FIT VECTORIZER (specific to feat_type & n_feats)\n","            # -------------------------------\n","            vectorizer_class, analyzer, ngram_range = vectorizer_params[feat_type]\n","            vectorizer = vectorizer_class(\n","                analyzer=analyzer,\n","                ngram_range=ngram_range,\n","                max_features=n_feats,\n","                lowercase=False,     # texts already pre-lowercased\n","                dtype=np.float32     # reduces memory usage & speeds up computation\n","            )\n","            X = vectorizer.fit_transform(texts)   # sparse matrix representation\n","\n","            # -------------------------------\n","            # DEFINE PIPELINE (scaler + classifier)\n","            # -------------------------------\n","            pipe = Pipeline([\n","                ('scaler', StandardScaler(with_mean=False)),          # preserves sparse format\n","                ('classifier', svm.SVC(probability=True))            # SVM classifier\n","            ])\n","\n","            # -------------------------------\n","            # RANDOMIZED SEARCH CONFIGURATION\n","            # -------------------------------\n","            param_grid = {\n","                'classifier__C': c_options,\n","                'classifier__kernel': kernel_options\n","            }\n","\n","            # define scoring metrics for evaluation\n","            scoring = {\n","                'accuracy_score': make_scorer(accuracy_score),\n","                'precision_score': make_scorer(precision_score, average='macro', zero_division=0),\n","                'recall_score': make_scorer(recall_score, average='macro', zero_division=0),\n","                'f1_score': make_scorer(f1_score, average='macro', zero_division=0)\n","            }\n","\n","            # setup RandomizedSearchCV\n","            grid = RandomizedSearchCV(\n","                pipe,\n","                param_distributions=param_grid,\n","                n_iter=n_iter_random,               # number of random combos to try\n","                cv=cv,                               # stratified CV\n","                n_jobs=-1,                           # use all CPUs\n","                scoring=scoring,                     # multiple metrics\n","                refit='f1_score',              # best model selected by accuracy\n","                verbose=0,\n","                error_score=np.nan                   # assign NaN on failure, avoid tracebacks\n","            )\n","\n","            # -------------------------------\n","            # FIT MODEL ON TRAINING DATA\n","            # -------------------------------\n","            grid.fit(X, y_train)\n","            results = grid.cv_results_\n","\n","            # -------------------------------\n","            # STORE CV RESULTS\n","            # -------------------------------\n","            for idx, params in enumerate(results['params']):\n","                # store mean scores for current parameter combination\n","                all_grid_scores.append((\n","                    results['mean_test_accuracy_score'][idx],\n","                    results['mean_test_precision_score'][idx],\n","                    results['mean_test_recall_score'][idx],\n","                    results['mean_test_f1_score'][idx]\n","                ))\n","                # store corresponding parameter settings\n","                all_parameter_combos.append((\n","                    feat_type, n_feats, sample_len,\n","                    params['classifier__C'], params['classifier__kernel']\n","                ))\n","\n","# -------------------------------\n","# CREATE REPORT DATAFRAME\n","# -------------------------------\n","import pandas as pd\n","full_report = []\n","for (acc, prec, rec, f1), params in zip(all_grid_scores, all_parameter_combos):\n","    model_name = '-'.join([str(i) for i in params])   # create unique model name from parameters\n","    full_report.append((model_name, acc, prec, rec, f1))\n","\n","# convert to DataFrame and sort by f1 score\n","df = pd.DataFrame(full_report, columns=['model', 'accuracy', 'precision', 'recall', 'f1 score'])\n","df_sorted = df.sort_values(by='f1 score', ascending=False)\n","print(df_sorted)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}