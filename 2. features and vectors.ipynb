{"cells":[{"cell_type":"markdown","id":"000a13a9-b999-4bdb-95b8-118107fde2b7","metadata":{"id":"000a13a9-b999-4bdb-95b8-118107fde2b7"},"source":["# 2. Features and Vectors"]},{"cell_type":"markdown","id":"09195a41-bcbf-4eb5-8eca-a31e62e8a747","metadata":{"id":"09195a41-bcbf-4eb5-8eca-a31e62e8a747"},"source":["## 2.1 Extraction and Vectorization\n","\n","In stylometry, feature vectors are numerical representations of text segments and their lexical composition.  Hence, vectors guarantee an objective basis for comparability of texts. The power of this, is that algorithms can process textual characteristics automatically, simultaneously, fast and on a large scale, often disregarding word order (the **bag-of-words** approach). A vector α, filled with the feature frequencies of text α, after all, becomes a solid basis for comparison with vectors β, γ, ... which in their own turn represent the stylistic properties of text samples β, γ, ... This common ground allows to approach stylistic difference literally as **mathematical difference** and (as we will see when visualizing the data) **geometric distance**.\n","\n","Before we get started, first run the code below again (copy of preprocessing code from the previous notebook) in order to make our text data manipulable."]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"W-hj5bBgORWW"},"id":"W-hj5bBgORWW","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c75c9fb7-48b5-424f-822b-744885804a13","metadata":{"id":"c75c9fb7-48b5-424f-822b-744885804a13"},"outputs":[],"source":["import re\n","import glob\n","from string import punctuation\n","\n","# Declare empty lists to fill up with our metadata and data\n","authors, titles, texts = [], [], []\n","\n","# We declare some parameters — the 'settings' of our stylometric experiments\n","sample_len = 1400 # word length of text segment\n","\n","# Function to clean and split text\n","def clean_and_split_text(text, sample_len):\n","    words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()\n","    return [words[i:i + sample_len] for i in range(0, len(words), sample_len)]\n","\n","for filename in uploaded.keys():\n","    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n","    with open(filename, encoding='utf-8-sig') as file:\n","        text = file.read().strip()\n","        bulk = clean_and_split_text(text, sample_len)\n","\n","        for index, sample in enumerate(bulk):\n","            if len(sample) == sample_len:\n","                authors.append(author)\n","                titles.append(f\"{title}_{index + 1}\")\n","                texts.append(\" \".join(sample))\n","\n","# Print summary to confirm things worked\n","print(\"Text processing complete!\")\n","print(f\"Number of text segments: {len(texts)}\")\n","print(f\"Number of authors: {len(set(authors))}\")\n","print(f\"Number of titles/segments: {len(titles)}\")\n","\n","# Optional: print a sample segment\n","print(\"\\nSample processed text segment:\\n\", texts[0][:200], \"...\")"]},{"cell_type":"markdown","id":"fc8c1fa9-03a1-412c-b935-2a70eb65e24d","metadata":{"id":"fc8c1fa9-03a1-412c-b935-2a70eb65e24d"},"source":["The **[scikit-learn](https://scikit-learn.org/stable/index.html) \"Machine Learning in Python\"-library** offers tremendously handy modules and transformers to extract features in a format supported by machine learning algorithms.\n","\n","Below, we initialize our first vectorizer, under the variable `model`, which helps extract the textual features we are interested in (`word`, `char`, meaning respectively 'word' or 'character'), the number of features (`max_features=20`), but which can also be helpful to eliminate words that are irrelevant to our focus (`stop_words`), or where we can feed a restrictive list of words in advance that narrows the selection down (`vocabulary`) would be a comprehensive list containing those features which you want to look at exclusively). The `n_gram` range, consequently, formulates the sequences of *n* characters / words / parts-of-speech one wants to look at."]},{"cell_type":"code","execution_count":null,"id":"91910329-5626-445f-8252-c4ad6c8e057c","metadata":{"id":"91910329-5626-445f-8252-c4ad6c8e057c"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Initialize sklearn vectorizer model\n","model = CountVectorizer(stop_words=[], # filter words\n","                        max_features=20, # n features = vector length / vector dimensionality.\n","                        analyzer='word', # feature type\n","                        vocabulary=None, # takes list of fixed words\n","                        ngram_range=((1,1)))\n","\n","X = model.fit_transform(texts).toarray()\n","\n","# Print confirmation\n","print(\"Vectorization complete!\")\n","print(f\"Number of text segments: {X.shape[0]}\")\n","print(f\"Vector dimensionality / features: {X.shape[1]}\")\n","print(\"\\nFeature names:\", model.get_feature_names_out())\n","\n","print(X)"]},{"cell_type":"markdown","id":"507437a8-4f7d-45f6-a789-f3a1ea196105","metadata":{"id":"507437a8-4f7d-45f6-a789-f3a1ea196105"},"source":["The result of this process, the matrix `X` printed above, is our very first matrix containing vectors representing the feature frequencies of each of our text segments. The `toarray()` converts X into a standard array format, typically `numpy`-array.  \n","In `X`, each row equals a text segment. Every column stands for the frequency of an extracted feature.\n","\n","In the code block below, we make the above more intuitive by representing `X`in a so-called DataFrame (abbreviated as `df`) using `pandas`. This is an open-source data analysis and manipulation library in Python that provides data structures and functions needed to manipulate structured data."]},{"cell_type":"code","execution_count":null,"id":"9da772c5-9e2f-40a4-9cbb-ba667a6da389","metadata":{"id":"9da772c5-9e2f-40a4-9cbb-ba667a6da389"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.DataFrame(X) # structures matrix X as a DataFrame\n","features = model.get_feature_names_out()\n","\n","# Assigns labels to colu`mns and rows (i.e. features and title segments)\n","df.columns = features # assigns column labels\n","df.index = [title for title in titles] # assigns row labels\n","\n","print(df)"]},{"cell_type":"markdown","id":"bd3e06e1-cd4f-4cf1-8148-1ba151c2cb08","metadata":{"id":"bd3e06e1-cd4f-4cf1-8148-1ba151c2cb08"},"source":["At this point, the scikit-learn feature extraction model still sorts the columns by alphabet rather than by frequency. The code below shows you how to refit your vectorizer so the vectors, somewhat more intuitively, descend from higher to lower frequency."]},{"cell_type":"code","execution_count":null,"id":"accd0761-2b78-4b54-a928-8fcc56e0d205","metadata":{"id":"accd0761-2b78-4b54-a928-8fcc56e0d205"},"outputs":[],"source":["import numpy as np\n","\n","# Sum the feat frequencies across all documents (i.e. text segments)\n","feat_frequencies = np.asarray(X.sum(axis=0)).flatten()\n","\n","# Get the feature names (features)\n","features = model.get_feature_names_out()\n","\n","# Create a DataFrame with features and their frequencies\n","feat_freq_df = pd.DataFrame({'feature': features, 'frequency': feat_frequencies})\n","\n","# Sort the DataFrame by frequency in descending order\n","feat_freq_df = feat_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n","\n","# Get the sorted features\n","sorted_features = feat_freq_df['feature'].tolist()\n","\n","# Reorder the columns of the matrix X according to the sorted features\n","sorted_indices = [model.vocabulary_[feat] for feat in sorted_features]\n","X_sorted = X[:, sorted_indices]\n","\n","# Print the sorted feature names and their corresponding frequencies\n","print(feat_freq_df)\n","print(sorted_features)"]},{"cell_type":"markdown","id":"45156d94-6b44-4743-a7c1-934cd2768068","metadata":{"id":"45156d94-6b44-4743-a7c1-934cd2768068"},"source":["Now you can refit a new `CountVectorizer` on our texts by feeding the `sorted_features`container to the `vocab` parameter of the vectorizer."]},{"cell_type":"code","execution_count":null,"id":"1f6fc9f1-5ea1-4abb-8392-8dd50e508da7","metadata":{"id":"1f6fc9f1-5ea1-4abb-8392-8dd50e508da7"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Initialize sklearn model\n","model = CountVectorizer(stop_words=[],\n","                        max_features=20,\n","                        analyzer='word',\n","                        vocabulary=sorted_features,\n","                        ngram_range=((1,1)))\n","\n","X = model.fit_transform(texts).toarray()\n","\n","df = pd.DataFrame(X)\n","features = model.get_feature_names_out()\n","\n","# Assigns names to columns and features (i.e. features and title segments)\n","df.columns = features\n","df.index = [title for title in titles]\n","\n","print(df)"]},{"cell_type":"markdown","id":"22b4c390-1548-4666-98b1-bb3e8580c90e","metadata":{"id":"22b4c390-1548-4666-98b1-bb3e8580c90e"},"source":["Try to work with different settings, and then look at the various results in the extraction and vectorization process."]},{"cell_type":"markdown","id":"a1793b27-73e6-470c-94f0-3c1f53d1556f","metadata":{"id":"a1793b27-73e6-470c-94f0-3c1f53d1556f"},"source":["Below we introduce another common type of vectorization called **TF-IDF-vectorization**, which stands for ‘term frequency-inverse document frequency’, see C. D. Manning, P. Raghavan, and H. Schütze, *Introduction to Information Retrieval* (New York, Cambridge University Press, 2008), at 289-290. It divides all feature values by the number of documents that respective feature appears in. As a consequence, less common features receive a higher weight, which prevents them from sinking away (and from losing statistical significance) amidst more common features. You can try to fit a TfidfVectorizer to your own dataset by running the code block below."]},{"cell_type":"code","execution_count":null,"id":"0349ae3d-00a3-4258-b54c-441caa7c4b44","metadata":{"id":"0349ae3d-00a3-4258-b54c-441caa7c4b44"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Initialize sklearn model\n","model = TfidfVectorizer(stop_words=[],\n","                        max_features=20,\n","                        analyzer='word',\n","                        vocabulary=sorted_features,\n","                        ngram_range=((1,1)))\n","\n","X = model.fit_transform(texts).toarray()\n","\n","df = pd.DataFrame(X)\n","features = model.get_feature_names_out()\n","\n","# Assigns names to columns and features (i.e. features and title segments)\n","df.columns = features\n","df.index = [title for title in titles]\n","\n","print(df)"]},{"cell_type":"markdown","id":"456a5db1-b277-4452-8ff4-fe5d627616fb","metadata":{"id":"456a5db1-b277-4452-8ff4-fe5d627616fb"},"source":["## 2.2 Distance Measures and Scaling\n","\n","### 2.2.1 Understanding Distance: Burrows' Delta\n","\n","Popularly applied to look at distances between vectors in the recent history of computational stylistics, are **John Burrows’** distance measures **Delta** (2002), **Zeta** and **Iota**. Burrows' methods, especially Delta, proved invaluable in the development of stylometry in its own time, and continues to provide an intuitive introduction to stylometric research. Due to the proliferation and improved understanding of the mathematical-statistical precepts underlying his theory, applying Delta by itself is not often carried out anymore and has become a tad outdated. We will explain why below.\n","\n","With Delta, Burrows essentially operationalized and matched two data-analytical steps: he first **'normalized'** or **'scaled'** the frequencies of words, and afterward calculated a **distance** between these vectors. The scaling implied that Burrows ‘standardized’ the word frequencies over the whole corpus so that the mean for each word is 0, and the standard deviation is 1, also known as the ‘z-score’. Consequently, he applied the so-called 'Manhattan' distance, to be treated further below.\n","\n","It turns out that Burrows' way of normalizing and measuring distance was, however, just one of many ways in which to conceptualize the distance between vectors. Argamon (2008) showed that, despite it being very effective, it may even not necessarily be the most optimal way. There have been several proposals to improve Delta (Hoover 2004b, Argamon 2008, Eder, Smith and Aldridge 2011, Kestemont and Rybicki 2013).\n","\n","If we want to implement Burrows' Delta in Python, we have to normalize our text vectors in `X` by transforming them to z scores, which can easily be done by importing `StandardScaler()` from `sklearn`. Consequently, we can apply the distance metric **Manhattan** (`sklearn.metrics.pairwise`) to calculate the similarity between the data points of a test text and a target corpus. Note that we should not necessarily opt for Manhattan distance. Other distance metrics, especially **Euclidean** and **Cosine**, provide valuable routes that can (and in fact should) be tested in performance. There are quite a few distance measures (or distance functions) around, but arguably they are are all variations falling under one of these three commonly encountered measures.\n","\n","* **Manhattan** or ‘taxicab distance’ has its onomasty in a city street plan, is a distance function which measures two points along axes at right angles, indeed, as if one would be navigating in a city along city blocks.\n","* **Euclidean** distance is commonly the most straightforward way of going from one place to another, and can be generally thought of as the distance in which ‘the crow flies,’ directly fixing one point to another without taking any turns.\n","* **Cosine** instead measures the cosine of the angle between the points."]},{"cell_type":"markdown","id":"da7e4221-a722-4923-b632-94d9a02add2b","metadata":{"id":"da7e4221-a722-4923-b632-94d9a02add2b"},"source":["### 2.2.2 Scaling\n","\n","More often than not, analysis will be carried out not on raw frequencies of features, but on ‘normalized’ or ‘weighted’ frequencies (feature scaling). In fact, TF-IDF is arguably already a (be it subtle) way of scaling feature frequencies.\n","\n","Scaling data is crucial in all statistical methods. It transforms the data to ensure that different features are on a similar scale. Key advantages are improved performance of data-analytical operations at a later stage, the equal importance (weight) granted by each of the respective features in the dataset, and a more intuitive interpretability of the results.\n","\n","With `sklearn.preprocessing` it is possible to import scaling models such as `Normalizer`, `StandardScaler`, `MinMaxScaler`, and many others, to apply scaling. The block of code below is an illustration."]},{"cell_type":"code","execution_count":null,"id":"4f114d64-b913-48c5-a2af-2877f3180936","metadata":{"scrolled":true,"id":"4f114d64-b913-48c5-a2af-2877f3180936"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","np.set_printoptions(suppress=True, precision=2) # suppresses scientific notation\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(X_scaled)"]},{"cell_type":"markdown","id":"13e066fc-af3e-42ad-9988-4a3b185264e9","metadata":{"id":"13e066fc-af3e-42ad-9988-4a3b185264e9"},"source":["Once we have scaled vectors, various distance measures can be applied. The block of code below yields a matrix  of pairwise differences (with a dimension or `np.shape` of 250x250) between the text segment and all text segments in the corpus, including... itself! Obviously, the distance is 0 in that case.\n","\n","Have a go and run the block of code below. For the first time, we will also use a plot (a heatmap) to visualize what is going on, making use of the Python library `matplotlib`. The heatmap visualizes the Manhattan distances between our text segment vectors. Each cell in the heatmap represents the distance between two vectors, with the color intensity indicating the magnitude of the distance. Darker colors correspond to smaller distances (i.e., more similar vectors), while lighter colors indicate larger distances (i.e., less similar vectors), see the color bar on the right provides. Both the x- and y-axes represent the indices of the vectors, making it possible to identify which vectors are being compared in each cell."]},{"cell_type":"code","execution_count":null,"id":"25127ca0-f56a-4823-986c-b39f549ed11f","metadata":{"scrolled":true,"id":"25127ca0-f56a-4823-986c-b39f549ed11f"},"outputs":[],"source":["from sklearn.metrics.pairwise import manhattan_distances\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","distance_matrix = manhattan_distances(X_scaled)\n","print(distance_matrix)\n","\n","# Plot a heatmap\n","fig = plt.figure()\n","plt.imshow(distance_matrix, cmap='hot', interpolation='nearest')\n","plt.colorbar(label='Distance')\n","plt.title('Heatmap of Vector Distances')\n","plt.xlabel('Vector Index')\n","plt.ylabel('Vector Index')\n","plt.show()"]},{"cell_type":"markdown","id":"df1d5e0e-3a46-4d8d-b99a-ee78787c4065","metadata":{"id":"df1d5e0e-3a46-4d8d-b99a-ee78787c4065"},"source":["## 2.3 Feature Weighting and Feature Selection\n","\n","Not to be confused with feature extraction, **feature selection** techniques penalize / filter out those features that are considered redundant, and reward or include only the best discriminants.\n","\n","Sometimes, the selection or elimination of features is done manually on the basis of **qualitative** arguments (Kestemont et al. 2015:206). However, most feature selection base themselves on **quantitative** criteria (where a distinction is made between filter-based and wrapper-based methods).\n","\n","The classes in the `sklearn.feature_selection` are useful for feature selection/dimensionality reduction to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets. Machine-learning techniques, to be treated later today, can be particularly helpful in evaluating the efficiency and relevance of including or excluding certain features. This relevance is often determined by **weights**: establishing feature weights is often a first step toward a pruned and more efficient set of discriminants.\n","\n","The idea that some words deserve less or more attention (again, deserve more **weight**) than others in attribution disputes is an idea that arose fairly early in stylometric research. Proposed by Hoover (2004), for instance, was '**culling**', where features that do not appear in all the representative texts of a corpus have to be disregarded.\n","\n","### 2.3.1 An Intuition through Burrows' and Craig's Zeta\n","\n","Above, we already mentioned Burrows's '**Zeta**' in passing. Zeta was first proposed in 2007 and modified by other scholars (Argamon 2008, Craig and Kinney 2009, Hoover 2004a, 2004b). In what follows, we will explain Zeta just as one of many possible ways to think about feature selection. Two main reasons: (1) Zeta is a commonly encountered state-of-the-art technique stylometric research, and (2) unlike other feature selection methods, there is less available Python code lying around for it in libraries or modules.\n","\n","John Burrows motivated the advantage of Zeta mainly from the idea that the frequency of a feature by itself can be misleading when talking about its significance for the author in question. It is, for instance, very possible that a certain word in a text occurs many times in the author's Prologue, but then again, for instance, fails to turn up later in the text. what we also need to take into account, he argued, is the **dispersion** of the word over a text or entire oeuvre. Instead of look at the frequencies of words, we concentrate on their consistency of appearance.\n","\n","Some key ideas to take home:\n","* The original Zeta compared subcorpora by two and not more authors, **Author A** and **Author B** below, and we will do the same here.\n","* Zeta is about frequency per segment (**proportion** or **ratio**) rather than raw, total word count.\n","* Zeta is mainly about 'distinctiveness', i.e. **preference and avoidance** by Authors A or B: Zeta returns a list of words that are statistically either preferred or avoided in each subcorpus.\n","* As a consequence, we see that Zeta analysis often excludes the extremely common words that are traditionally the focus of stylometry, and concentrates on the **middle of the word frequency spectrum**, yielding a few extremely efficient features for making distinction between Author A and Author B.\n","* Sample or **segment length** is an important parameter in its performance.\n","\n","As an additional parameter, you can also set a threshold: e.g. *w* needs to occur at least once per segment for one of both authors under consideration, before we consider it to have importance.\n","\n","Zeta combines the ratio of the sections by one author in which each word occurs with the ratio of the sections by the other author into a single measure of 'distinctiveness' for each word. It does so by subtracting the zeta scores of Author B from that of Author A. This 'composite score' produces two lists of words: one favored by the first author and avoided by the second, the other favored by the second author and avoided by the first.\n","\n","Concretely, this means:\n","* words that score `0` are used by exactly the same frequency per segment by both Authors A and B.\n","* words that score `-1` are used by Author A in every segment, and not in any segment by Author B.\n","* words that score `1` are used by Author B in every segment, and not in any segment by Author A.\n","\n","The code block below simply rehearses a lot we have seen earlier, only this time, we **vectorize the entire vocabulary** in a long feature vector, instead of only looking at the most common words."]},{"cell_type":"code","execution_count":null,"id":"de598366-1b28-494d-8c63-1b09494a9f53","metadata":{"id":"de598366-1b28-494d-8c63-1b09494a9f53"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from string import punctuation\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","# Declare empty lists to fill up with our metadata and data\n","authors = []\n","titles = []\n","texts = []\n","\n","# We declare some parameters — the 'settings' of our stylometric experiments\n","sample_len = 1400 # word length of text segment\n","\n","# Open all file objects in folder and gather data\n","for filename in uploaded.keys():\n","    author = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n","    title = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n","\n","    bulk = []\n","    text = open(filename, encoding='utf-8-sig').read() # utf-8-sig encoding automatically handles and removes Unicode Byte Order Mark (BOM) if present\n","\n","    # .split() method splits string into list of substrings based on a specified delimiter. By default, the delimiter is a whitespace\n","    # .strip() method removes leading and trailing whitespace from a string: spaces, tabs, newlines, and other whitespace characters.\n","    for word in text.strip().split():\n","        word = re.sub(r'\\d+', '', word) # escapes digits\n","        word = re.sub('[%s]' % re.escape(punctuation), '', word) # escape punctuation\n","        word = word.lower() # convert upper to lowercase\n","        bulk.append(word)\n","\n","    # Split up the text into discrete chunks or segments\n","    bulk = [word for word in bulk if word != \"\"] # list comprehension that removes emptry strings\n","    bulk = [bulk[i:i+sample_len] for i in range(0, len(bulk), sample_len)]\n","    for index, sample in enumerate(bulk):\n","        if len(sample) == sample_len:\n","            authors.append(author)\n","            titles.append(title + \"_{}\".format(str(index + 1)))\n","            texts.append(\" \".join(sample))\n","\n","# In this vectorizer, for the purpose of Zeta, no max_features is given.\n","model = CountVectorizer(stop_words=[], # filter words\n","                        analyzer='word', # feature type\n","                        vocabulary=None, # takes list of fixed words\n","                        ngram_range=((1,1)))\n","\n","X = model.fit_transform(texts).toarray()\n","features = model.get_feature_names_out()\n","\n","print(features)\n","print(X.shape) # prints dimension (n_samples, n_feats)"]},{"cell_type":"markdown","id":"89482691-4622-4644-bf34-8e462d2f6f19","metadata":{"id":"89482691-4622-4644-bf34-8e462d2f6f19"},"source":["In the code block below, we first have to indicate which authors we are interested in by typing out the author names (corresponding to the file names) in the list variable `authors_of_interest`.\n","\n","Consequently, we loop over matrix `X`, taking into account only the text segment vectors of interest, and crunch them into `bit_vectors`, or 'binarize them', in the sense that any frequency greater than `0` is simply converted into `1`.\n","\n","We can consequently count (as we do in the line of code containing `counts = np.sum(bit_vector)`) how many times the word occurs more than 0 in each text segment. If we divide this number by the variable `n_segments`, we get a `ratio`.\n"]},{"cell_type":"code","execution_count":null,"id":"7a0f9595-8f48-4c1a-9ae9-8ac3ecd86d30","metadata":{"id":"7a0f9595-8f48-4c1a-9ae9-8ac3ecd86d30"},"outputs":[],"source":["# Select the two authors we want to compare\n","authors_of_interest = ['Hildegardis-Bingensis', 'Bernardus-Claraevallensis']\n","\n","# Initialize a dictionary to store bit vectors for each author\n","new_X = {author: [] for author in authors_of_interest}\n","\n","# Convert feature counts to binary presence/absence for the selected authors\n","for author, title, x_vec in zip(authors, titles, X):\n","    if author in authors_of_interest:\n","        x_vec[x_vec > 0] = 1  # convert counts > 0 to 1 (presence/absence)\n","        new_X[author].append(x_vec)\n","\n","# Convert lists of vectors to numpy arrays for easier computation\n","new_X = {author: np.array(bit_matrix) for author, bit_matrix in new_X.items()}\n","\n","# Compute per-word ratios: in how many segments each word appears for each author\n","ratios_dict = {author: None for author in authors_of_interest}\n","for author, bit_matrix in new_X.items():\n","    n_segments = bit_matrix.shape[0]  # number of text segments for this author\n","    all_ratios = []\n","    for feat, bit_vector in zip(features, bit_matrix.transpose()):  # transpose: iterate over words\n","        counts = np.sum(bit_vector)  # number of segments in which the word occurs\n","        ratio = counts / n_segments   # ratio of segments with the word\n","        all_ratios.append(ratio)\n","    ratios_dict[author] = all_ratios  # store ratios per author\n","\n","# Prepare matrix for Zeta calculation\n","# negative -1: preference by Author A\n","# 0: same usage by both authors\n","# positive +1: preference by Author B\n","X_ratios = [all_ratios for all_ratios in ratios_dict.values()]\n","X_ratios = np.array(X_ratios).transpose()\n","y = [author for author in ratios_dict.keys()]\n","\n","# Calculate Zeta: difference in segment ratios between authors\n","zeta_scores = X_ratios[:, 1] - X_ratios[:, 0]\n","\n","# Sort words by Zeta scores\n","negatives = [(x, weight) for weight, x in sorted(zip(zeta_scores, features))]  # most negative = preferred by Author A\n","positives = negatives[::-1]  # most positive = preferred by Author B\n","\n","# Show top 10 distinctive words for each author\n","print(negatives[:10])\n","print(positives[:10])"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.patches import Patch\n","from matplotlib.cm import ScalarMappable\n","from matplotlib.colors import Normalize\n","\n","# --- Build the word list as you had it ---\n","top_neg_words = [word for word, _ in negatives[:10]]\n","top_pos_words = [word for word, _ in positives[:10]]\n","top_words = top_neg_words + top_pos_words\n","word_indices = [list(features).index(w) for w in top_words]\n","\n","# --- Distinctiveness (opacity) from |zeta| ---\n","# We normalize |zeta| across the selected top words to map into [min_alpha, 1.0]\n","zeta_for_top = np.abs(zeta_scores[word_indices])\n","zeta_max = zeta_for_top.max() if len(zeta_for_top) > 0 else 0.0\n","min_alpha = 0.15  # ensures low-distinctiveness words are still faintly visible\n","\n","if zeta_max == 0:\n","    # If everything is equally non-distinctive, use a constant faint alpha\n","    alphas_per_word = np.full(len(word_indices), min_alpha, dtype=float)\n","else:\n","    alphas_per_word = min_alpha + (zeta_for_top / zeta_max) * (1.0 - min_alpha)\n","\n","# --- Colors per author (colorblind-friendly) ---\n","palette = sns.color_palette(\"colorblind\", n_colors=len(authors_of_interest))\n","author_colors = {a: palette[i] for i, a in enumerate(authors_of_interest)}\n","\n","# --- Plot ---\n","fig, axes = plt.subplots(1, len(authors_of_interest), figsize=(16, 8), sharey=True)\n","\n","n_words = len(top_words)\n","for i, author in enumerate(authors_of_interest):\n","    ax = axes[i] if isinstance(axes, np.ndarray) else axes\n","\n","    # Get presence/absence for this author's segments on the selected words\n","    # new_X[author]: (n_segments, n_features)\n","    # We need (n_words, n_segments) for plotting with words on y-axis\n","    bit_matrix = new_X[author]\n","    sub = bit_matrix[:, word_indices].T.astype(float)  # shape: (n_words, n_segments)\n","\n","    # --- Build RGBA image ---\n","    # base color is author-specific; alpha per cell = presence * alpha(word)\n","    r, g, b = author_colors[author]\n","    # broadcast RGB over the grid\n","    rgb = np.dstack([\n","        np.full_like(sub, r, dtype=float),\n","        np.full_like(sub, g, dtype=float),\n","        np.full_like(sub, b, dtype=float),\n","    ])\n","    # alpha per word, broadcast across segments, masked by presence (sub is 0/1)\n","    alpha = sub * alphas_per_word[:, None]\n","    rgba = np.dstack([rgb, alpha])\n","\n","    im = ax.imshow(rgba, aspect=\"auto\", interpolation=\"none\", origin=\"upper\")\n","\n","    # Axes formatting\n","    ax.set_title(author)\n","    ax.set_xlabel(\"Segment\")\n","\n","    # Set y-ticks to the combined list of words\n","    axes[0].set_yticks(np.arange(len(top_words)) + 0.5)\n","    axes[0].set_yticklabels(top_words, rotation=0)\n","\n","    # Keep words oriented top-to-bottom matching list order\n","    ax.set_ylim(n_words - 0.5, -0.5)\n","\n","plt.suptitle(\"Word presence across segments — color by author, opacity by |zeta|\")\n","plt.tight_layout(rect=[0, 0, 1, 0.95])\n","plt.show()"],"metadata":{"id":"kKQ3FJXcJLw-"},"id":"kKQ3FJXcJLw-","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"343bfb42-bcdf-4f3f-b7b5-2b36949d93f1","metadata":{"id":"343bfb42-bcdf-4f3f-b7b5-2b36949d93f1"},"source":["As you can imagine, these words are powerfull stuff! They hand us quite effectively a number of discriminants that work very well in distinguishing two authors' lexical preferences from one another, and the usefulness of the discriminants is not confined by membership in the high frequency strata.\n","\n","Below, we visualize our results by using `matplotlib` once more.\n","\n","The variable `n_visualized` stands for 'number visualized': indicate how many of the top discriminants you wish to plot.\n","\n","Consequently, take a good look at this plot, and try to verify if this makes sense to you on a qualitative level!"]},{"cell_type":"code","execution_count":null,"id":"62888a52-f80a-4884-b77d-2c237e3cb384","metadata":{"id":"62888a52-f80a-4884-b77d-2c237e3cb384"},"outputs":[],"source":["from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","# Parameters\n","n_visualized = 10 # number of terms based on Zeta score visualized\n","\n","# Plot\n","fig, ax = plt.subplots()\n","\n","# store Zeta data in variables\n","x_negatives = [tup[1] for tup in negatives[:n_visualized]]\n","x_positives = [tup[1] for tup in positives[:n_visualized]][::-1]\n","y = [i for i in range(0, n_visualized )]\n","\n","# plot the values\n","for x_coord, y_coord in zip(y, x_positives):\n","    ax.barh(x_coord, y_coord, color='g', edgecolor='black', alpha=np.abs(y_coord))\n","for x_coord, y_coord in zip(y, x_negatives):\n","    ax.barh(x_coord, y_coord, color='r', edgecolor='black', alpha=np.abs(y_coord))\n","\n","# labelling the axes\n","ax2 = ax.twinx()\n","ax2.set_ylim(ax.get_ylim())\n","bottom_labels = [tup[0] for tup in negatives[:n_visualized]]\n","top_labels = [tup[0] for tup in positives[:n_visualized][::-1]]\n","ax.set_yticks(y)\n","ax.set_yticklabels(bottom_labels)\n","ax2.set_yticks(y)\n","ax2.set_yticklabels(top_labels)\n","\n","plt.title('Zeta scores')\n","\n","# Despine both axes\n","for spine in ['right', 'top', 'left', 'bottom']:\n","    ax.spines[spine].set_visible(False)\n","    ax2.spines[spine].set_visible(False)\n","\n","plt.axvline(x=0, lw=2, c='black', zorder=1)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}